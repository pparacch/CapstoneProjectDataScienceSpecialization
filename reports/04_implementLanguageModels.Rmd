---
title: "Implement Language Models"
author: "Pier Lorenzo Paracchini"
date: "09 mai 2016"
output: 
  html_document: 
    keep_md: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# WINDOWS LOCALE SETTING
Sys.setlocale(category = "LC_ALL",locale = "English_United States.1252")

require(profr)
require(hash)
```

# Collapsing the ngrams data into a single list for uni, bi and tri 

A single set of list for all of the Corpora for 
unigrams,
Bigrams and
Trigrams

```{r collapse, eval=F}
rm(list = ls())
source("./../scripts/model.R")
tw.4g <- load.twitter.4g.data(file_url_allTermsFrequency = "./../data/processed/02_01_s05_sampleCorpus_twitter_termsfrequency.4g.Rdata",
                              file_url_tdm = "./../data/processed/02_01_s04_sampleCorpus_twitter_tdm.4g.Rdata")

n.4g <- load.news.4g.data(file_url_allTermsFrequency = "./../data/processed/02_02_s05_sampleCorpus_news_termsfrequency.4g.Rdata",
                              file_url_tdm = "./../data/processed/02_02_s04_sampleCorpus_news_tdm.4g.Rdata")

b.4g <- load.blogs.4g.data(file_url_allTermsFrequency = "./../data/processed/02_03_s05_sampleCorpus_blogs_termsfrequency.4g.Rdata",
                              file_url_tdm = "./../data/processed/02_03_s04_sampleCorpus_blogs_tdm.4g.Rdata")

filename <- collapseToOneList(twitter.ng = tw.4g, news.dg = n.4g, blogs.ng = b.4g, ng = 4)

load(filename)
x <- d.ng.df$terms[grep("'{2,}", d.ng.df$terms)]
head(x)
tail(x)


rm(list = ls())
source("./../scripts/model.R")
tw.3g <- load.twitter.3g.data(file_url_allTermsFrequency = "./../data/processed/02_01_s05_sampleCorpus_twitter_termsfrequency.3g.Rdata",
                              file_url_tdm = "./../data/processed/02_01_s04_sampleCorpus_twitter_tdm.3g.Rdata")

n.3g <- load.news.3g.data(file_url_allTermsFrequency = "./../data/processed/02_02_s05_sampleCorpus_news_termsfrequency.3g.Rdata",
                              file_url_tdm = "./../data/processed/02_02_s04_sampleCorpus_news_tdm.3g.Rdata")

b.3g <- load.blogs.3g.data(file_url_allTermsFrequency = "./../data/processed/02_03_s05_sampleCorpus_blogs_termsfrequency.3g.Rdata",
                              file_url_tdm = "./../data/processed/02_03_s04_sampleCorpus_blogs_tdm.3g.Rdata")

filename <- collapseToOneList(twitter.ng = tw.3g, news.dg = n.3g, blogs.ng = b.3g, ng = 3)

load(filename)
x <- d.ng.df$terms[grep("'{2,}", d.ng.df$terms)]
head(x)
tail(x)

rm(list = ls())
source("./../scripts/model.R")

tw.2g <- load.twitter.2g.data(file_url_allTermsFrequency = "./../data/processed/02_01_s05_sampleCorpus_twitter_termsfrequency.2g.Rdata",
                              file_url_tdm = "./../data/processed/02_01_s04_sampleCorpus_twitter_tdm.2g.Rdata")

n.2g <- load.news.2g.data(file_url_allTermsFrequency = "./../data/processed/02_02_s05_sampleCorpus_news_termsfrequency.2g.Rdata",
                              file_url_tdm = "./../data/processed/02_02_s04_sampleCorpus_news_tdm.2g.Rdata")

b.2g <- load.blogs.2g.data(file_url_allTermsFrequency = "./../data/processed/02_03_s05_sampleCorpus_blogs_termsfrequency.2g.Rdata",
                              file_url_tdm = "./../data/processed/02_03_s04_sampleCorpus_blogs_tdm.2g.Rdata")

filename <- collapseToOneList(twitter.ng = tw.2g, news.dg = n.2g, blogs.ng = b.2g, ng = 2)

load(filename)
x <- d.ng.df$terms[grep("'{2,}", d.ng.df$terms)]
head(x)
tail(x)

rm(list = ls())
source("./../scripts/model.R")

tw.1g <- load.twitter.1g.data(file_url_allTermsFrequency = "./../data/processed/02_01_s05_sampleCorpus_twitter_termsfrequency.1g.Rdata",
                              file_url_tdm = "./../data/processed/02_01_s04_sampleCorpus_twitter_tdm.1g.Rdata")

n.1g <- load.news.1g.data(file_url_allTermsFrequency = "./../data/processed/02_02_s05_sampleCorpus_news_termsfrequency.1g.Rdata",
                              file_url_tdm = "./../data/processed/02_02_s04_sampleCorpus_news_tdm.1g.Rdata")

b.1g <- load.blogs.1g.data(file_url_allTermsFrequency = "./../data/processed/02_03_s05_sampleCorpus_blogs_termsfrequency.1g.Rdata",
                              file_url_tdm = "./../data/processed/02_03_s04_sampleCorpus_blogs_tdm.1g.Rdata")

filename <- collapseToOneList(twitter.ng = tw.1g, news.dg = n.1g, blogs.ng = b.1g, ng = 1)

load(filename)
x <- d.ng.df$terms[grep("'{2,}", d.ng.df$terms)]
head(x)
tail(x)
```


# Language Model Creation - some Notes & The Process

# 1st Approach Trigram Models

See `./trigramModel` folder & `model_trigramLanguageModel.R` file


Initial approach -> create a simplify data structure for the trigram model
bigram, nextWord, counter, probability

The processing time is demanding and for this reason the size of element processed has been ad-hoc selected to a certain number of element (most frequent first). Still we have three models one for each corpora - Twitter, News and Blog. The three model shoudl be collapsed into only one table.

Process

- generate the model (twitter, news, blogs) / need to focus ON PERFORMANCE OF THE CODE _(NOT VERY EFFICIENT)_
- merge the models into 1 big model (TODO)
- evaluate the model (probability/ Perplexity on a common test data set)

__Note the generation of the models depends on the normalized coprpora so when re-cleaning & processing the corpora all of those steps need to be performed.__


```{r trigramModelAllCorporaModelCreation, eval = F}
rm(list = ls())
source("./../scripts/model.R")
source("./../scripts/model_trigramLanguageModel.R")

#noElements <- 1000
##VERY INEFFICIENT -> OPTIMIZED CODE generate.corpora.3gramModel.i
##THAT IS THE REASON WHY WE WERE LIMITING THE NUMBER OF ELEMENTS
# filename <- generate.corpora.3gramModel(noElements = noElements)
# load(file = filename)
# tmp1 <- data.3g.model.df


#For the Creation of the model
filename <- generate.corpora.3gramModel.i()
load(file = filename)

```

```{r modelUsage}
rm(list = ls())
source("./../scripts/model.R")
source("./../scripts/model_trigramLanguageModel.R")

#For using the existent Model - the file below contains teh latest generated model
load(file = "./../scripts/trigramLanguageModel/allCorpora.3gModel_i_2016-05-10.rdata")
bigram <- "<s> thanks"
data.3g.model.df[which(data.3g.model.df$bigram == bigram),]

#The guy in front of me just bought a pound of bacon, a bouquet, and a case of
# beer(g)
bigram <- "case of"
data.3g.model.df[which(data.3g.model.df$bigram == bigram),]

# You're the reason why I smile everyday. Can you follow me please? It would mean the
# world
bigram <- "mean the"
data.3g.model.df[which(data.3g.model.df$bigram == bigram),]

# Hey sunshine, can you follow me and make me the
# Happiest(g)
bigram <- "me the"
data.3g.model.df[which(data.3g.model.df$bigram == bigram),]

# Very early observations on the Bills game: Offense still struggling but the
# defense(g)
bigram <- "but the"
data.3g.model.df[which(data.3g.model.df$bigram == bigram),]

# Go on a romantic date at the
# beach(g)
bigram <- "at the"
data.3g.model.df[which(data.3g.model.df$bigram == bigram),]

# Well I'm pretty sure my granny has some old bagpipes in her garage I'll dust them off and be on my
# way(g)
bigram <- "on my"
data.3g.model.df[which(data.3g.model.df$bigram == bigram),]

# Ohhhhh #PointBreak is on tomorrow. Love that film and haven't seen it in quite some
# time
bigram <- "quite some"
data.3g.model.df[which(data.3g.model.df$bigram == bigram),]

# After the ice bucket challenge Louis will push his long wet hair out of his eyes with his little
# fingers(g)
bigram <- "his little"
data.3g.model.df[which(data.3g.model.df$bigram == bigram),]

# Be grateful for the good times and keep the faith during the
# bad(g)
bigram <- "during the"
data.3g.model.df[which(data.3g.model.df$bigram == bigram),]

# If this isn't the cutest thing you've ever seen, then you must be
# insane(g)
bigram <- "must be"
data.3g.model.df[which(data.3g.model.df$bigram == bigram),]

sentenceProbability <- function(model.df, trigrams){
    size <- length(trigrams)
    result <- 0
    for(i in 1:size){
        el.words <- ngramTokenize(trigrams[i],1)
        el.b <- paste(el.words[1], el.words[2])
        el.nw <- el.words[3]
        el.p <- model.df$probability[
            which(model.df$bigram == el.b & model.df$next.word == el.nw)]
        print(paste(el.b,"::", el.nw, "::", el.p, sep = ""))
        result <- result * el.p
    }
    result
}

#haven't seen it in quite some time
x <- c("one of the", "seen it in", "a lot of", "it in quite", "in quite some", "quite some time")
sentenceProbability(model.df = data.3g.model.df, trigrams = x)

bigram <- "haven't seen"
data.3g.model.df[which(data.3g.model.df$bigram == bigram),]

bigram <- "quite some"
data.3g.model.df[which(data.3g.model.df$bigram == bigram),]
```


```{r trigramModels, eval = F}
# #OLD APPROACH ONE MODEL FOR EACH CORPUS
# #Code has been not mantained
# #Twitter
# rm(list = ls())
# source("./../scripts/model.R")
# source("./../scripts/model_trigramLanguageModel.R")
# 
# 
# noElements = 100000
# filename.3g <- generate.twitter.3gramModel(folder = "./../data/processed/",
#                             file.allTermFrequency.3g = "twitter.sample1.allTermsFrequency.3g.Rdata",
#                             file.tdm.3g = "twitter.sample1.tdm.3g.Rdata",
#                             file.allTermFrequency.2g = "twitter.sample1.allTermsFrequency.2g.Rdata",
#                             file.tdm.2g = "twitter.sample1.tdm.2g.Rdata",
#                             noElements = noElements)
# 
# 
# # load("./trigramLanguageModel/twitter.3gModel_2016-05-07.rdata")
# # data.3g.tc.info$terms[1:10]
# # data.3g.tc.info$counters[1:10]
# # data.2g.tc.info$terms[1:10]
# # data.2g.tc.info$counters[1:10]
# # data.3g.model.df[1:10,]
# 
# ## NEWS Corpora
# rm(list = ls())
# source("./model_trigramLanguageModel.R")
# 
# noElements = 100000
# filename.3g <- generate.news.3gramModel(folder = "./../data/processed/",
#                             file.allTermFrequency.3g = "news.sample.allTermsFrequency.3g.Rdata",
#                             file.tdm.3g = "news.sample.tdm.3g.Rdata",
#                             file.allTermFrequency.2g = "news.sample.allTermsFrequency.2g.Rdata",
#                             file.tdm.2g = "news.sample.tdm.2g.Rdata",
#                             noElements = noElements)
# 
# # load("./trigramLanguageModel/news.3gModel_2016-05-07.rdata")
# # data.3g.tc.info$terms[1:10]
# # data.3g.tc.info$counters[1:10]
# # data.2g.tc.info$terms[1:10]
# # data.2g.tc.info$counters[1:10]
# # data.3g.model.df[1:10,]
# 
# 
# ## BLOGS Corpora
# rm(list = ls())
# source("./model_trigramLanguageModel.R")
# 
# noElements = 100000
# filename.3g <- generate.blogs.3gramModel(folder = "./../data/processed/",
#                             file.allTermFrequency.3g = "blogs.sample.allTermsFrequency.3g.Rdata",
#                             file.tdm.3g = "blogs.sample.tdm.3g.Rdata",
#                             file.allTermFrequency.2g = "blogs.sample.allTermsFrequency.2g.Rdata",
#                             file.tdm.2g = "blogs.sample.tdm.2g.Rdata",
#                             noElements = noElements)
# 
# # load("./trigramLanguageModel/blogs.3gModel_2016-05-07.rdata")
# # data.3g.tc.info$terms[1:10]
# # data.3g.tc.info$counters[1:10]
# # data.2g.tc.info$terms[1:10]
# # data.2g.tc.info$counters[1:10]
# # data.3g.model.df[1:10,]
# 
# rm(list = ls())
```


```{r useModels, eval = F}
# #OLD APPROACH ONE MODEL FOR EACH CORPUS
# #Code has been not mantained
# 
# rm(list = ls())
# source("model.R")
# #(b.3g.model, b.3g.model.df, file = "./tmp/models/blogs.3g.model_10000topTerms.Rdata")
# load(file = "./tmp/models/blogs.3g.model_10000topTerms.Rdata")
# 
# #(n.3g.model, n.3g.model.df, file = "./tmp/models/news.3g.model_10000topTerms.Rdata")
# load(file = "./tmp/models/news.3g.model_10000topTerms.Rdata")
# 
# # (tw.3g.model, tw.3g.model.df, file = "./tmp/models/twitter.3g.model_10000topTerms.Rdata")
# load(file = "./tmp/models/twitter.3g.model_10000topTerms.Rdata")
# 
# bigram <- "<s> thanks"
# tw.3g.model.df[which(tw.3g.model.df$tw.3g.model.t.bigram == bigram),]
# n.3g.model.df[which(n.3g.model.df$tw.3g.model.t.bigram == bigram),]
# b.3g.model.df[which(b.3g.model.df$tw.3g.model.t.bigram == bigram),]
# 
# #The guy in front of me just bought a pound of bacon, a bouquet, and a case of
# # beer(g)
# bigram <- "case of"
# tw.3g.model.df[which(tw.3g.model.df$tw.3g.model.t.bigram == bigram),]
# n.3g.model.df[which(n.3g.model.df$tw.3g.model.t.bigram == bigram),]
# b.3g.model.df[which(b.3g.model.df$tw.3g.model.t.bigram == bigram),]
# 
# # You're the reason why I smile everyday. Can you follow me please? It would mean the
# # world
# bigram <- "mean the"
# tw.3g.model.df[which(tw.3g.model.df$tw.3g.model.t.bigram == bigram),]
# n.3g.model.df[which(n.3g.model.df$tw.3g.model.t.bigram == bigram),]
# b.3g.model.df[which(b.3g.model.df$tw.3g.model.t.bigram == bigram),]
# 
# # Hey sunshine, can you follow me and make me the
# # Happiest(g)
# bigram <- "me the"
# tw.3g.model.df[which(tw.3g.model.df$tw.3g.model.t.bigram == bigram),]
# n.3g.model.df[which(n.3g.model.df$tw.3g.model.t.bigram == bigram),]
# b.3g.model.df[which(b.3g.model.df$tw.3g.model.t.bigram == bigram),]
# 
# # Very early observations on the Bills game: Offense still struggling but the
# # defense(g)
# bigram <- "but the"
# tw.3g.model.df[which(tw.3g.model.df$tw.3g.model.t.bigram == bigram),]
# n.3g.model.df[which(n.3g.model.df$tw.3g.model.t.bigram == bigram),]
# b.3g.model.df[which(b.3g.model.df$tw.3g.model.t.bigram == bigram),]
# 
# # Go on a romantic date at the
# # beach(g)
# bigram <- "at the"
# tw.3g.model.df[which(tw.3g.model.df$tw.3g.model.t.bigram == bigram),]
# n.3g.model.df[which(n.3g.model.df$tw.3g.model.t.bigram == bigram),]
# b.3g.model.df[which(b.3g.model.df$tw.3g.model.t.bigram == bigram),]
# 
# # Well I'm pretty sure my granny has some old bagpipes in her garage I'll dust them off and be on my
# # way(g)
# bigram <- "on my"
# tw.3g.model.df[which(tw.3g.model.df$tw.3g.model.t.bigram == bigram),]
# n.3g.model.df[which(n.3g.model.df$tw.3g.model.t.bigram == bigram),]
# b.3g.model.df[which(b.3g.model.df$tw.3g.model.t.bigram == bigram),]
# 
# # Ohhhhh #PointBreak is on tomorrow. Love that film and haven't seen it in quite some
# # time
# bigram <- "quite some"
# tw.3g.model.df[which(tw.3g.model.df$tw.3g.model.t.bigram == bigram),]
# n.3g.model.df[which(n.3g.model.df$tw.3g.model.t.bigram == bigram),]
# b.3g.model.df[which(b.3g.model.df$tw.3g.model.t.bigram == bigram),]
# 
# 
# # After the ice bucket challenge Louis will push his long wet hair out of his eyes with his little
# # fingers(g)
# bigram <- "his little"
# tw.3g.model.df[which(tw.3g.model.df$tw.3g.model.t.bigram == bigram),]
# n.3g.model.df[which(n.3g.model.df$tw.3g.model.t.bigram == bigram),]
# b.3g.model.df[which(b.3g.model.df$tw.3g.model.t.bigram == bigram),]
# 
# 
# # Be grateful for the good times and keep the faith during the
# # bad(g)
# bigram <- "during the"
# tw.3g.model.df[which(tw.3g.model.df$tw.3g.model.t.bigram == bigram),]
# n.3g.model.df[which(n.3g.model.df$tw.3g.model.t.bigram == bigram),]
# b.3g.model.df[which(b.3g.model.df$tw.3g.model.t.bigram == bigram),]
# 
# 
# # If this isn't the cutest thing you've ever seen, then you must be
# # insane(g)
# bigram <- "must be"
# tw.3g.model.df[which(tw.3g.model.df$tw.3g.model.t.bigram == bigram),]
# n.3g.model.df[which(n.3g.model.df$tw.3g.model.t.bigram == bigram),]
# b.3g.model.df[which(b.3g.model.df$tw.3g.model.t.bigram == bigram),]
# 
# 
# 
# sentenceProbability <- function(model.df, trigrams){
#     size <- length(trigrams)
#     result <- 0
#     for(i in 1:size){
#         el.words <- ngramTokenize(trigrams[i],1)
#         el.b <- paste(el.words[1], el.words[2])
#         el.nw <- el.words[3]
#         el.p <- model.df$tw.3g.model.t.probability[
#             which(model.df$tw.3g.model.t.bigram == el.b & model.df$tw.3g.model.t.nextWord == el.nw)]
#         print(paste(el.b,"::", el.nw, "::", el.p, sep = ""))
#         result <- result * el.p
#     }
#     result
# }
# 
# #haven't seen it in quite some time
# x <- c("haven't seen it", "seen it in", "it in quite", "in quite some", "quite some time")
# sentenceProbability(model.df = tw.3g.model.df, trigrams = x)
# sentenceProbability(model.df = n.3g.model.df, trigrams = x)
# sentenceProbability(model.df = b.3g.model.df, trigrams = x)
# 
# bigram <- "haven't seen"
# tw.3g.model.df[which(tw.3g.model.df$tw.3g.model.t.bigram == bigram),]
# n.3g.model.df[which(n.3g.model.df$tw.3g.model.t.bigram == bigram),]
# b.3g.model.df[which(b.3g.model.df$tw.3g.model.t.bigram == bigram),]
# 
# bigram <- "quite some"
# tw.3g.model.df[which(tw.3g.model.df$tw.3g.model.t.bigram == bigram),]
# n.3g.model.df[which(n.3g.model.df$tw.3g.model.t.bigram == bigram),]
# b.3g.model.df[which(b.3g.model.df$tw.3g.model.t.bigram == bigram),]
# 
```

# Another Approach for the model - Use the terms/ counters vector directly

## StupidBackoff model

## SImple experiment using only twitter data using the stupid backoff algorithm

```{r}
rm(list = ls())
source("./../scripts/model.R")
require(hash)

visualizeCumuluativeCoverage <- function(freqs, terms, title){
    idx <- order(freqs, decreasing = T)
    terms.term <- terms[idx]
    terms.freq <- freqs[idx]

    terms.count <- sum(terms.freq)
    terms.unique <- length(unique(terms.term))
    terms.cumulativeCoverage <- (cumsum(terms.freq)/ terms.count) * 100
    
    
    plot(terms.cumulativeCoverage, type = "l", xlab = "Number Of Words", ylab = "% Coverage", main = title)
    abline(v = which(terms.cumulativeCoverage >= 50)[1], col = "orange", lwd = 3)
    abline(v = which(terms.cumulativeCoverage >= 90)[1], col = "red", lwd = 3)
    legend(x = "topright", lty=c(1,1), lwd=c(3,3), col=c("orange", "red"), legend = c("50% coverage", "90% coverage")) 
}


load("./../data/processed/04_s01_allCorpora_aggregated_termsFrequency.3g.rdata")
allCorpora.3g <- d.ng.df
d.ng.df <- NULL
visualizeCumuluativeCoverage(allCorpora.3g$total, allCorpora.3g$terms, "allCorpora-3g")

load("./../data/processed/04_s01_allCorpora_aggregated_termsFrequency.2g.rdata")
allCorpora.2g <- d.ng.df
d.ng.df <- NULL
visualizeCumuluativeCoverage(allCorpora.2g$total, allCorpora.2g$terms, "allCorpora-2g")

load("./../data/processed/04_s01_allCorpora_aggregated_termsFrequency.1g.rdata")
allCorpora.1g <- d.ng.df
d.ng.df <- NULL
visualizeCumuluativeCoverage(allCorpora.1g$total, allCorpora.1g$terms, "allCorpora-1g")

#Size of the object in memory
format(object.size(allCorpora.3g$terms), "auto")
format(object.size(allCorpora.3g$total), "auto")

format(object.size(allCorpora.2g$terms), "auto")
format(object.size(allCorpora.2g$total), "auto")

format(object.size(allCorpora.1g$terms), "auto")
format(object.size(allCorpora.1g$total), "auto")

#Something to investigate the use of hash for memory reduction
#CAREFULL do not try with big object - a circa 400 M object size (especially the saving)
allCorpora.3g.h <- hash(allCorpora.3g$terms, allCorpora.3g$total)
format(object.size(allCorpora.3g.h), "auto")
save(allCorpora.3g.h, file = "./../scripts/stupidBackoff_3g_LanguageModel/04_allCorpora_3g_hash_object.rdata")
allCorpora.3g.h[allCorpora.3g$terms[1:10]]
allCorpora.3g$terms[1:10]
allCorpora.3g$total[1:10]

allCorpora.2g.h <- hash(allCorpora.2g$terms, allCorpora.2g$total)
format(object.size(allCorpora.2g.h), "auto")
save(allCorpora.2g.h, file = "./../scripts/stupidBackoff_3g_LanguageModel/04_allCorpora_2g_hash_object.rdata")

allCorpora.2g.h[allCorpora.2g$terms[1:10]]
allCorpora.2g$terms[1:10]
allCorpora.2g$total[1:10]

allCorpora.1g.h <- hash(allCorpora.1g$terms, allCorpora.1g$total)
format(object.size(allCorpora.1g.h), "auto")
save(allCorpora.1g.h, file = "./../scripts/stupidBackoff_3g_LanguageModel/04_allCorpora_1g_hash_object.rdata")
allCorpora.1g.h[allCorpora.1g$terms[1:10]]
allCorpora.1g$terms[1:10]
allCorpora.1g$total[1:10]

s <- "<s> <s> you will absolutely love it after the"

Rprof(tmp <- tempfile())
estimateSentenceProbabilities(s = s, t.terms = allCorpora.3g$terms, t.counters = allCorpora.3g$total,
                    b.terms = allCorpora.2g$terms, allCorpora.2g$total,
                    u.words = allCorpora.1g$terms, u.counters = allCorpora.1g$total)
Rprof(NULL)
summaryRprof(tmp)

s <- "<s> <s> you will absolutely love it after the the"

Rprof(tmp <- tempfile())
estimateSentenceProbabilities(s = s, t.terms = allCorpora.3g$terms, t.counters = allCorpora.3g$total,
                    b.terms = allCorpora.2g$terms, allCorpora.2g$total,
                    u.words = allCorpora.1g$terms, u.counters = allCorpora.1g$total)
Rprof(NULL)
summaryRprof(tmp)


s <- "if this isn't the cutest thing you've ever seen then you must be insane"
Rprof(tmp <- tempfile())
estimateSentenceProbabilities(s = s, t.terms = allCorpora.3g$terms, t.counters = allCorpora.3g$total,
                    b.terms = allCorpora.2g$terms, allCorpora.2g$total,
                    u.words = allCorpora.1g$terms, u.counters = allCorpora.1g$total)
Rprof(NULL)
summaryRprof(tmp)
```

