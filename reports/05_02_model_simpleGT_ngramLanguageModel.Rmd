---
title: 'Models: Comparing & Evaluation'
author: "Pier Lorenzo Paracchini"
date: "10 mai 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
require(knitr)

source("./../scripts/model_evaluation.R")
source("./../scripts/goodTuringSmoothing.R")
source("./../scripts/model_supportingFunctions.R")
source("./../scripts/model_simpleGTS_ngramLanguageModel.R")

# WINDOWS LOCALE SETTING
Sys.setlocale(category = "LC_ALL",locale = "English_United States.1252")
```

# Testing

A simple approach for testing the different models is to evaluate  __How does our language model prefer good sentences over bad one?__, or in other words __Does the model assign high probability to "real" or "frequently observed" sentences?__.

The test is done using a set of sentences that have not been used for training the model - basic assumption __the model has never seen those sentences__.

The evaluation metrics (ln scale) used to evaluate how well the model is preforming in the __P(S)__ (probability assigned to the sentence S = w1, w2, .., wn) and perplexity. These metrics are related __perplexity = f(probability)__. A better model is the one which assigns a higher probability to the words that actually occurs (reality).

```{r creatingTestDataSetOfSentences, collpase = T}
sentences
```

# Simplified Trigrams Model with (simple) Good-Turing Smoothing

```{r loadData_model1_1, cache = T}

load("./../data/processed/04_s01_allCorpora_aggregated_termsFrequency.3g.rdata")
allSample.termFreq.3g <- d.ng.df
d.ng.df <- NULL
```


```{r evaluate_model1_1, collapse = T}

model1_1.evaluation <- sapply(sentences, 
                              FUN = estimateSentenceProbabilities.ng.model.simple.withGoodTuring.smoothing, 
                              terms = allSample.termFreq.3g$terms, counters = allSample.termFreq.3g$total, ng = 3)
colnames(model1_1.evaluation) <- NULL
model1_1.evaluation <- t(model1_1.evaluation)
model1_1.perplexity <- calculatePerplexity(model1_1.evaluation)
```

`r kable(model1_1.evaluation, caption = "Tri-grams Model - Base")`

__Perplexity (log base2):__ `r model1_1.perplexity`

```{r}
rm(allSample.termFreq.3g, model1_1.evaluation)
```


## Simplified Bigrams Model with (simple) Good-Turing Smoothing

```{r loadData_model2, cache = T}

load("./../data/processed/04_s01_allCorpora_aggregated_termsFrequency.2g.rdata")
allSample.termFreq.2g <- d.ng.df
d.ng.df <- NULL
```


```{r evaluate_model2, collapse = T}
model2.evaluation <- sapply(sentences, 
                              FUN = estimateSentenceProbabilities.ng.model.simple.withGoodTuring.smoothing, 
                              terms = allSample.termFreq.2g$terms, counters = allSample.termFreq.2g$total, ng = 2)
colnames(model2.evaluation) <- NULL
model2.evaluation <- t(model2.evaluation)
model2.perplexity <- calculatePerplexity(model2.evaluation)
```

`r kable(model2.evaluation, caption = "Bi-grams Model - Base")`

__Perplexity (log base2):__ `r model2.perplexity`

```{r}
rm(allSample.termFreq.2g, model2.evaluation)
```


## Simplified Unigram Model with (simple) Good-Turing Smoothing

```{r loadData_model3, cache = T}

load("./../data/processed/04_s01_allCorpora_aggregated_termsFrequency.1g.rdata")
allSample.termFreq.1g <- d.ng.df
d.ng.df <- NULL
```


```{r evaluate_model3, collapse = T}
model3.evaluation <- sapply(sentences, 
                              FUN = estimateSentenceProbabilities.ng.model.simple.withGoodTuring.smoothing, 
                              terms = allSample.termFreq.1g$terms, counters = allSample.termFreq.1g$total, ng = 1)
colnames(model3.evaluation) <- NULL
model3.evaluation <- t(model3.evaluation)
model3.perplexity <- calculatePerplexity(model3.evaluation)
```

`r kable(model3.evaluation, caption = "Bi-grams Model - Base")`

__Perplexity (log base2):__ `r model3.perplexity`


Model | Perplexity
------------- | -------------
Trigram - GT smoothing - allCorpora | `r model1_1.perplexity`
Bigram - GT smoothing - allCorpora | `r model2.perplexity`
Unigram - GT smoothing - allCorpora | `r model3.perplexity`