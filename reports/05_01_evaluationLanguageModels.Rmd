---
title: 'Models: Comparing & Evaluation - Linear Interpolation'
author: "Pier Lorenzo Paracchini"
date: "10 mai 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
require(knitr)

source("./../scripts/model.R")
source("./../scripts/linearInterpolationModel.R")


# WINDOWS LOCALE SETTING
Sys.setlocale(category = "LC_ALL",locale = "English_United States.1252")

```

# Testing

A simple approach for testing the different models is to evaluate  __How does our language model prefer good sentences over bad one?__, or in other words __Does the model assign high probability to "real" or "frequently observed" sentences?__.

The test is done using a set of sentences that have not been used for training the model - basic assumption __the model has never seen those sentences__.

The evaluation metrics (ln scale) used to evaluate how well the model is preforming in the __P(S)__ (probability assigned to the sentence S = w1, w2, .., wn) and perplexity. These metrics are related __perplexity = f(probability)__. A better model is the one which assigns a higher probability to the words that actually occurs (reality).

```{r creatingTestDataSetOfSentences}
sentences <- c(
    #High Probability (max is ln(1) = 0/ Lower perplexity (limit to 0 when ln(1))
    "<s> you will absolutely love it </s>",
    "<s> you will absolutely love him after the meeting </s>",
    "<s> if this isn't the cutest thing you've ever seen then you must be insane </s>",
    "<s> i think most people mean things like that </s>",
    "<s> morning is my least favorite time of the day </s>",
    "<s> i love you </s>",
    "<s> it’s potentially the most important engagement of our week yet it’s the one the majority of us aren’t even penciling into our agendas </s>",
    
    #Lower Probability/ Higher perplexity
    "<s> you will absolutely love and it </s>",
    "<s> you will absolutely love him after the and meeting </s>",
    "<s> you will absolutely him after the meeting </s>",
    "<s> if this isn't not the cutest thing you've ever never seen then you must be insane </s>",
    "<s> i think most people mean things like that </s>",
    "<s> i love you not </s>"
)
```

# Linear Interpolation with Good-Turing Smoothing

See NLP classroom (Coursera)

```{r loadTheModel, cache = T, collapse=T}
load(file = "./../data/processed/04_s01_allCorpora_aggregated_termsFrequency.3g.rdata")
d.3g <- d.ng.df

load(file = "./../data/processed/04_s01_allCorpora_aggregated_termsFrequency.2g.rdata")
d.2g <- d.ng.df

load(file = "./../data/processed/04_s01_allCorpora_aggregated_termsFrequency.1g.rdata")
d.1g <- d.ng.df
d.ng.df <- NULL


#Size of the object in memory
format(object.size(d.3g$terms), "auto")
format(object.size(d.3g$total), "auto")

format(object.size(d.2g$terms), "auto")
format(object.size(d.2g$total), "auto")

format(object.size(d.1g$terms), "auto")
format(object.size(d.1g$total), "auto")

```


```{r testLinearInterpolation, collapse = T}

m4.l1 = 0.8; m4.l2 = 0.15; m4.l3 = 0.05
model4.evaluation <- sapply(sentences, FUN = estimateSentProb.linearInterpolation.model.withGoodTuring.smoothing, 
                            t.terms = d.3g$terms, t.counters = d.3g$total,
                            b.terms = d.2g$terms, b.counters = d.2g$total,
                            u.terms = d.1g$terms, u.counters = d.1g$total,
                            lambda_1 = m4.l1, lambda_2 = m4.l2, lambda_3 = m4.l3)

colnames(model4.evaluation) <- NULL
model4.evaluation <- t(model4.evaluation)
model4.perplexity <- calculatePerplexity(model4.evaluation)
```

`r kable(model4.evaluation, caption = "Linear Interpolation Model - Base")`

__Perplexity (log base2):__ `r model4.perplexity`


```{r testLinearInterpolation_1, collapse = T}

m5.l1 = 0.9; m5.l2 = 0.08; m5.l3 = 0.02
model5.evaluation <- sapply(sentences, FUN = estimateSentProb.linearInterpolation.model.withGoodTuring.smoothing, 
                            t.terms = d.3g$terms, t.counters = d.3g$total,
                            b.terms = d.2g$terms, b.counters = d.2g$total,
                            u.terms = d.1g$terms, u.counters = d.1g$total,
                            lambda_1 = m5.l1, lambda_2 = m5.l2, lambda_3 = m5.l3)

colnames(model5.evaluation) <- NULL
model5.evaluation <- t(model5.evaluation)
model5.perplexity <- calculatePerplexity(model5.evaluation)
```

```{r testLinearInterpolation_2, collapse = T}

m6.l1 = 0.3; m6.l2 = 0.6; m6.l3 = 0.1
model6.evaluation <- sapply(sentences, FUN = estimateSentProb.linearInterpolation.model.withGoodTuring.smoothing, 
                            t.terms = d.3g$terms, t.counters = d.3g$total,
                            b.terms = d.2g$terms, b.counters = d.2g$total,
                            u.terms = d.1g$terms, u.counters = d.1g$total,
                            lambda_1 = m6.l1, lambda_2 = m6.l2, lambda_3 = m6.l3)

colnames(model6.evaluation) <- NULL
model6.evaluation <- t(model6.evaluation)
model6.perplexity <- calculatePerplexity(model6.evaluation)
```

```{r testLinearInterpolation_3, collapse = T}

m7.l1 = 0.1; m7.l2 = 0.87; m7.l3 = 0.03
model7.evaluation <- sapply(sentences, FUN = estimateSentProb.linearInterpolation.model.withGoodTuring.smoothing, 
                            t.terms = d.3g$terms, t.counters = d.3g$total,
                            b.terms = d.2g$terms, b.counters = d.2g$total,
                            u.terms = d.1g$terms, u.counters = d.1g$total,
                            lambda_1 = m7.l1, lambda_2 = m7.l2, lambda_3 = m7.l3)

colnames(model7.evaluation) <- NULL
model7.evaluation <- t(model7.evaluation)
model7.perplexity <- calculatePerplexity(model7.evaluation)
```

`r kable(model7.evaluation, caption = "Linear Interpolation Model - Base")`

__Perplexity (log base2):__ `r model7.perplexity`


Model | lambda1 | lambda2 | lambda3 |Perplexity
------------- | -------------| -------------| -------------| -------------
LI - GT smoothing| `r m4.l1` | `r m4.l2` | `r m4.l3` |`r model4.perplexity`
LI - GT smoothing| `r m5.l1` | `r m5.l2` | `r m5.l3` |`r model5.perplexity`
LI - GT smoothing| `r m6.l1` | `r m6.l2` | `r m6.l3` |`r model6.perplexity`
LI - GT smoothing| `r m7.l1` | `r m7.l2` | `r m7.l3` |`r model7.perplexity`
