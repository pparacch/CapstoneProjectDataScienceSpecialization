---
title: 'Models: Comparing & Evaluation'
author: "Pier Lorenzo Paracchini"
date: "10 mai 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
require(knitr)

source("./../scripts/model_evaluation.R")

# WINDOWS LOCALE SETTING
Sys.setlocale(category = "LC_ALL",locale = "English_United States.1252")
```

# Testing

A simple approach for testing the different models is to evaluate  __How does our language model prefer good sentences over bad one?__, or in other words __Does the model assign high probability to "real" or "frequently observed" sentences?__.

The test is done using a set of sentences that have not been used for training the model - basic assumption __the model has never seen those sentences__.

The evaluation metrics (ln scale) used to evaluate how well the model is preforming in the __P(S)__ (probability assigned to the sentence S = w1, w2, .., wn) and perplexity. These metrics are related __perplexity = f(probability)__. A better model is the one which assigns a higher probability to the words that actually occurs (reality).

```{r creatingTestDataSetOfSentences, collpase = T}
sentences
```

# Tri-gram model based on MLE, using all of the trigrams and bigrams within all of the sample corpus, No smoothing

The trigrams and bigrams generated using all of the sample corpora are used. __If the combination (bigram, next word) is not found then the model assign a 0 probability to that specific case. Not possible to calculate the perplexity for the model.__   

```{r loadTheModel, cache = T}
#Model - based on all of the 3g and 2g from the all corpora used for training
#Model - simple MLE / no smoothing applied
load(file = "./../scripts/model_trigramLanguage/allCorpora.3gModel_i_2016-05-10.rdata")
```

# Based on MLE, using all of the trigrams and bigrams within all of the sample corpus, no smoothing

```{r testTrigramModel_3g_all_2g_all_MLE_model, collapse = T}
source("./../scripts/model_evaluation.R")
source("./../scripts/model_supportingFunctions.R")
source("./../scripts/model_trigramLanguageModel.R")

#Order elements in the data structure by count (decreasing)
data.3g.model.df <- data.3g.model.df[order(data.3g.model.df$count, decreasing = T),]

#Size of the object in memory
format(object.size(data.3g.model.df), "auto")

# estimateSentenceProbabilities.mle(paste("<s>", sentences[1], "</s>"), data.3g.model.df)
data.3g.model.evaluation <- sapply(sentences, FUN = estimateSentenceProbabilities.mle, data.3g.model.df)
colnames(data.3g.model.evaluation) <- NULL
data.3g.model.evaluation <- t(data.3g.model.evaluation)
model1.perplexity <- calculatePerplexity(data.3g.model.evaluation)
```

`r kable(data.3g.model.evaluation, caption = "Tri-grams Model - Base")`

__Perplexity (log base2):__ `r model1.perplexity`


Model | Perplexity
------------- | -------------
Trigram - no smoothing - all corpora| `r model1.perplexity`

# Experiments

* Add a smoothing technique in order to manage unknown word
* reduce the size of the trigram vocabulary - filter by counting (see cumulative coverage for the tigrams)
