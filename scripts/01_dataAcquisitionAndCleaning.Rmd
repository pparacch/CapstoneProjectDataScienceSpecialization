---
title: "Data Acquisition & Cleaning"
author: "Pier Lorenzo Paracchini"
date: "18 april 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
Sys.setlocale("LC_ALL", "C")
```

## Data Acquisition

Original data has been downloaded from the following [link](https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip)

## Loading the Data

Original data in _"\.\\data\\original\\final\\en\_US"_ - using the __"en_US"__ LOCALE.  

__THINKING POINT__ how to load the data?

__Tips, tricks, and hints from the Capstone Project material__

__Loading the data in (tip & trick)__

    'This dataset is fairly large. We emphasize that you don't necessarily need to load the entire dataset in to build your algorithms (see point 2 below). At least initially, you might want to use a smaller subset of the data. Reading in chunks or lines using R's readLines or scan functions can be useful. You can also loop over each line of text by embedding readLines within a for/while loop, but this may be slower than reading in large chunks at a time. Reading pieces of the file at a time will require the use of a file connection in R.'


```{r experimentWithTheLoadingOfOriginalData, collapse=T}
#setwd("./scripts")
con <- file("./../data/original/final/en_US/en_US.twitter.txt", "r") 
readLines(con, 1) ## Read the first line of text 
readLines(con, 1) ## Read the next line of text 
readLines(con, 5) ## Read in the next 5 lines of text 
close(con) ## It's important to close the connection when you are done

con <- file("./../data/original/final/en_US/en_US.blogs.txt", "r") 
readLines(con, 1) ## Read the first line of text 
readLines(con, 1) ## Read the next line of text 
readLines(con, 1) ## Read the next line of text 

close(con) ## It's important to close the connection when you are done

con <- file("./../data/original/final/en_US/en_US.news.txt", "r") 
readLines(con, 1) ## Read the first line of text 
readLines(con, 1) ## Read the next line of text 
readLines(con, 1) ## Read the next line of text 
close(con) ## It's important to close the connection when you are done
```

Original data:

* no headers
* one feature, the text (a twitter message, a blog or a piece of news)


__Sampling (tip & trick)__

    `To reiterate, to build models you don't need to load in and use all of the data. Often relatively few randomly selected rows or chunks need to be included to get an accurate approximation to results that would be obtained using all the data. Remember your inference class and how a representative sample can be used to infer facts about a population. You might want to create a separate sub-sample dataset by reading in a random subset of the original data and writing it out to a separate file. That way, you can store the sample and not have to recreate it every time. You can use the rbinom function to "flip a biased coin" to determine whether you sample a line of text or not.`

```{r dataReductionForExploration, collapse=T}
con <- file("./../data/original/final/en_US/en_US.twitter.txt", "r") 
data.twitter.all <- readLines(con, skipNul = T)
# data.twitter.all <- readLines(con, skipNul = F)
#Some lines have embedded NULLS - the lines are actually truncated if skipNul = F
# data.twitter.all[167155]
# data.twitter.all[167156]
close(con)

coin.biased.outcome <- rbinom(length(data.twitter.all), 1, 0.20)
table(coin.biased.outcome)

data.twitter.sample <- data.twitter.all[coin.biased.outcome == 1]
writeLines(data.twitter.sample, "./../data/processed/en_US.twitter.sample.txt")
```



## Left Over To Be Reviewed

```{r collapse=T}

require(R.utils)
con.bin <- file("./../data/original/final/en_US/en_US.twitter.txt", "rb") 
data.twitter.info <- countLines(con.bin)
close(con.bin)

noOfTwitters <- data.twitter.info[1]
data.twitter.sample.id <- sample(1:noOfTwitters, noOfTwitters * 0.001, replace = F)
```
