---
title: "Text Prediction - Milestone Report (Capstone Project)"
author: "Pier Lorenzo Paracchini"
date: "29 april 2016"
output: 
  html_document: 
    keep_md: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

#Required Libraries
require(knitr)

# WINDOWS LOCALE SETTING
Sys.setlocale(category = "LC_ALL",locale = "English_United States.1252")

getOption("encoding")
Sys.getlocale("LC_CTYPE")
localeToCharset()
```

# Synopsis

The main objective of this __Milestone Report__ is to display and explain only the major features of the data you have identified and briefly summarize your next" plans for creating the prediction algorithm and Shiny app behind the final product. Specifically as stated in the assignment description:

* Demonstrate that the data has been downloaded and loaded
* Create a basic report of summary statistics about the data sets (twitters, news, blogs)
* Report any interesting findings and eventual considerations/ implications 
* Get feedback on the "next" plans for creating a prediction algorithm and Shiny app

# The Data

The data is originated from a corpus called [HC Corpora](www.corpora.heliohost.org) and it can be downloded at the following [link](https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip). The corpora have been collected from publicly available sources by a web crawler and includes tweets, blogs and news in english, german, finnish and russian.

A meaningful excerpt from the [About the Corpora](http://webcache.googleusercontent.com/search?q=cache:dzpVyq5etNYJ:www.corpora.heliohost.org/aboutcorpus.html+&cd=3&hl=en&ct=clnk&gl=us) informational page:

    ' You may still find lines of entirely different languages in the corpus. There are 2 main reasons for that: 1. Similar languages. Some languages are very similar, and the automatic language checker could therefore erroneously accept the foreign language text. 2. "Embedded" foreign languages. While a text may be mainly in the desired language there may be parts of it in another language. Since the text is then split up into individual lines, it is possible to see entire lines written in a foreign language.Whereas number 1 is just an out-and-out error, I think number 2 is actually desirable, as it will give a picture of when foreign language is used within the main language.'

Note! The focus of the analysis is on the __english language only ('en_US')__ - covering: tweets (twitter), news and blogs.

```{r dataLoad, echo = F, cache = T}
con <- file("./../data/original/final/en_US/en_US.twitter.txt", "r") 
data.twitter.all <- readLines(con, skipNul = T)
close(con)

con <- file("./../data/original/final/en_US/en_US.news.txt", "r") 
data.news.all <- readLines(con, skipNul = T)
close(con)

con <- file("./../data/original/final/en_US/en_US.blogs.txt", "r") 
data.blogs.all <- readLines(con, skipNul = T)
close(con)

data.twitter.all.nchar <- nchar(data.twitter.all)
data.news.all.nchar <- nchar(data.news.all)
data.blogs.all.nchar <- nchar(data.blogs.all)


#Some Statistic about the original files
data.sources <- c("twitters", "news", "blogs")
data.noOfLines <- c(length(data.twitter.all), length(data.news.all), length(data.blogs.all))
data.maxNoOfChars <- c(max(data.twitter.all.nchar), max(data.news.all.nchar), max(data.blogs.all.nchar))
data.minNoOfChars <- c(min(data.twitter.all.nchar), min(data.news.all.nchar), min(data.blogs.all.nchar))

data.info.df <- data.frame(sources = data.sources, 
                           noOfLines = data.noOfLines, 
                           maxNoOfChar = data.maxNoOfChars,
                           minNoOfChar = data.minNoOfChars)
```


`r kable(data.info.df, caption = "Some basic statistics about the Corpora")`

__Some considerations__

* the __amount of data available__ considering the tweets, news and blogs entries. For simplification, the __exploration__ has been done using a representative sample to infer facts about a population (considering also limitations connected with the available processing hardware)

* the minimum size, in terms of number of characters, for the different entries. There are tweets, news and blogs with few characters - __are they relevant__?

# Cleaning the Data

## Encoding Issues (Gremlings)

When loading the data the following locale/ encoding has been used __`r Sys.getlocale("LC_CTYPE")` \ `r localeToCharset()`__. Inspecting the loaded data it is possible to identify some encoding issues (gremlings) due to the unrecognized characters (not supported languages, emoticons, etc)

    '`r data.twitter.all[15]`'
    '`r data.twitter.all[10967]`'
    '`r data.twitter.all[7046]`'


In order to remove such gremlings the following __strategy__ and __simplification__ has been considered: limit the set of available characters to the __ASCII__ charset, removing non ASCII characters.


```{r removeGremlings, collapse = T, echo = F}
##Using iconv to replace non ASCII char with an empty char
data.twitter.all.ascii <-  iconv(data.twitter.all, from = localeToCharset(), to = "ASCII", "")
data.news.all.ascii <-  iconv(data.news.all, from = localeToCharset(), to = "ASCII", "")
data.blogs.all.ascii <-  iconv(data.blogs.all, from = localeToCharset(), to = "ASCII", "")


##Reset working data after removing Gremlings
data.twitter.all <- data.twitter.all.ascii
data.twitter.all.nchar <- nchar(data.twitter.all)

data.news.all <- data.news.all.ascii
data.news.all.nchar <- nchar(data.news.all)

data.blogs.all <- data.blogs.all.ascii
data.blogs.all.nchar <- nchar(data.blogs.all)
```

    '`r data.twitter.all[15]`'
    '`r data.twitter.all[10967]`'
    '`r data.twitter.all[7046]`'

## Entries with a limited number of chars

### Twitter Corpora

```{r tweetsDistribution, echo = F}
hist(nchar(data.twitter.all), main = "No Of Chars Distribution - Tweets", xlab = "no of characters")

shortTweets.maxNoOfChar <- 20
```

There are around __`r sum(data.twitter.all.nchar <= shortTweets.maxNoOfChar)` tweets__ (__`r round(sum(data.twitter.all.nchar <= shortTweets.maxNoOfChar)/ length(data.twitter.all),2)`%__) that are less than __`r shortTweets.maxNoOfChar`__ chars long. Few examples of such tweets can be found below:

```{r tweetsShortAnalysis, collapse=T, echo=F}
#Find tweets that are less than 
twitter.lessThanOr3chars <- data.twitter.all[data.twitter.all.nchar <= shortTweets.maxNoOfChar]
twitter.lessThanOr3chars[100:110]
```

Because of the limited number of such tweets and the "irrelevance" of their content (especially the ones with less than 10 chars), it has been decided to remove them from the __twitter corpora__.

```{r tweetsShortRemoval, collapse=T, echo = F}
#Removing such short tweets
data.twitter.all <- data.twitter.all[data.twitter.all.nchar > 20]
data.twitter.all.nchar <- nchar(data.twitter.all)

#twitter.df <- data.frame(text = data.twitter.all, nchar = data.twitter.all.nchar)
```

### News Corpora

```{r newsDistribution, echo = F}
#Let's focus on news
a <- data.news.all.nchar < 1500
par(mfrow=c(1,2))
hist(nchar(data.news.all[a]), main = "No Of Chars per News (< 1500)", breaks = 100, xlab = "no of characters")
hist(nchar(data.news.all[!a]), main = "No Of Chars per News (>= 1500)", breaks = 100, xlab = "no of characters")

shortNews.maxNoOfChar <- 20
```

There are around __`r sum(data.news.all.nchar <= shortNews.maxNoOfChar)` news__ (__`r round(sum(data.news.all.nchar <= shortNews.maxNoOfChar)/ length(data.news.all),2)`%__) that are less than __`r shortNews.maxNoOfChar`__ chars long. Few examples of such news can be found below:

```{r newsShortAnalysis, echo = F}
a <- data.news.all.nchar <= shortNews.maxNoOfChar
head(data.news.all[a], 10)
```

Because of the limited number of such news and the "irrelevance" of their content , it has been decided to remove them from the __news corpora__.

```{r newsShortRemoval, echo = F}
#Removing such short news
data.news.all <- data.news.all[data.news.all.nchar > shortNews.maxNoOfChar]
data.news.all.nchar <- nchar(data.news.all)

#news.df <- data.frame(text = data.news.all, nchar = data.news.all.nchar)
```

### Blogs Corpora

```{r blogsDistribution, echo = F}
a <- data.blogs.all.nchar < 1300
par(mfrow=c(1,2))
hist(nchar(data.blogs.all[a]), main = "No Of Chars per Blogs (< 1300)", xlab = "no of characters")
hist(nchar(data.blogs.all[!a]), main = "No Of Chars per Blogs (>= 1300)", breaks = 10000, xlab = "no of characters")

shortBlogs.maxNoOfChar <- 20
```

There are around __`r sum(data.blogs.all.nchar <= shortBlogs.maxNoOfChar)` blogs__ (__`r round(sum(data.blogs.all.nchar <= shortBlogs.maxNoOfChar)/ length(data.blogs.all),2)`%__) that are less than __`r shortBlogs.maxNoOfChar` chars__ long. Few examples of such blogs can be found below:

```{r blogsShortAnalysis, echo = F}
a <- data.blogs.all.nchar <= shortBlogs.maxNoOfChar
head(data.blogs.all[a], 10)
```

Because of the limited number of such blogs and the "irrelevance" of their content , it has been decided to remove them from the __blogs corpora__.

```{r blogsShortRemoval, echo = F}
#Removing such short blogss
data.blogs.all <- data.blogs.all[data.blogs.all.nchar > shortBlogs.maxNoOfChar]
data.blogs.all.nchar <- nchar(data.blogs.all)

#blogs.df <- data.frame(text = data.blogs.all, nchar = data.blogs.all.nchar)

data1.sources <- c("twitters", "news", "blogs")
data1.noOfLines <- c(length(data.twitter.all), length(data.news.all), length(data.blogs.all))
data1.maxNoOfChars <- c(max(data.twitter.all.nchar), max(data.news.all.nchar), max(data.blogs.all.nchar))
data1.minNoOfChars <- c(min(data.twitter.all.nchar), min(data.news.all.nchar), min(data.blogs.all.nchar))

data1.info.df <- data.frame(sources = data1.sources, 
                           noOfLines = data1.noOfLines, 
                           maxNoOfChar = data1.maxNoOfChars,
                           minNoOfChar = data1.minNoOfChars)
```

## Removal of Profanity Words

```{r profanityEntries, echo=F}
twitters.g1Spam <- grepl("WET TSHIRT", data.twitter.all, ignore.case = F)
twitters.sex <- grepl("[[:space:]]sex[a-z]*[[:space:]]", data.twitter.all, ignore.case = T)
```

Note that the data contain words of offensive and profane meaning. Some examples ...

    `r data.twitter.all[twitters.g1Spam][1]`
    `r data.twitter.all[twitters.sex][2]`

```{r loadProfanityWords, echo = F}
con <- file("./bad-words.txt", "r") 
stopwords.profanityWords <- readLines(con, skipNul = T)
close(con)
```

Profanity words will be removed from the Corpora (as stopwords). An external [resource](http://www.cs.cmu.edu/~biglou/resources/) providing a comprehensive list of __`r length(stopwords.profanityWords)` profanity words__ is used.

## Others 

* replace contractions __"u, r, c'mon, doin', y'all, ya'll, ma'am"__ with __"you, are, come on, doing, you all, madam"__
* remove __links__ (e.g. "https://www.coursera.org/" or "http://www.coursera.org/")
* remove __"RT"__ (only for the twitter corpora)

```{r othersRemoval, echo = F, cache = T}
replaceContraction <- function(texts, contraction, replaceWith, ignoreCase = F){
    gsub(pattern = contraction, replacement = replaceWith, x = texts, ignore.case = ignoreCase)
}

remove_RT_retweetted <- function(texts, ignoreCase = T){
    a <- gsub(pattern = " RT :? ?", replacement = " ", x = texts, ignore.case = ignoreCase)
    gsub(pattern = "^RT ", replacement = "", x = a, ignore.case = ignoreCase)
}

remove_links <- function(texts, ignoreCase = T){
    gsub(pattern = "^(https?:\\/\\/)?([\\da-z\\.-]+)\\.([a-z\\.]{2,6})([\\/\\w \\.-]*)*\\/?$", 
         replacement = " ", x = texts, ignore.case = ignoreCase)
}


remove_contractions <- function(theTexts){
    rem.contr.tmp <- replaceContraction(texts = theTexts, contraction = " u ", replaceWith = " you ", ignoreCase = T)
    rem.contr.tmp <- replaceContraction(texts = theTexts, contraction = " r ", replaceWith = " are ", ignoreCase = T)
    rem.contr.tmp <- replaceContraction(texts = theTexts, contraction = "c'mon", replaceWith = "come on",ignoreCase = T)
    rem.contr.tmp <- replaceContraction(texts = theTexts, contraction = "doin'", replaceWith = "doing",ignoreCase = T)
    rem.contr.tmp <- replaceContraction(texts = theTexts, contraction = "[yY]a?'a?ll", replaceWith = "you all",ignoreCase = T)
    rem.contr.tmp <- replaceContraction(texts = theTexts, contraction = "ma'am", replaceWith = "madam",ignoreCase = T)
    rem.contr.tmp
}

# test.u <- "I'm coo... Jus at work hella tired are u ever in cali"
# test.u.expected <- "I'm coo... Jus at work hella tired are you ever in cali"
# test.u.expected == replaceContraction(texts = test.u, contraction = " u ", replaceWith = " you ", ignoreCase = T)
# 
# test.r <- "I'm coo... Jus at work hella tired r you ever in cali"
# test.r.expected <- "I'm coo... Jus at work hella tired are you ever in cali"
# test.r.expected == replaceContraction(texts = test.r, contraction = " r ", replaceWith = " are ", ignoreCase = T)
# 
# test.RT <- c("I'm cool... RT : Just at work RT tired r you ever in cali", "RT I'm cool...")
# test.RT.expected <- c("I'm cool... Just at work tired r you ever in cali", "I'm cool...")
# result <- remove_RT_retweetted(test.RT)
# test.RT.expected[1] == result[1]
# test.RT.expected[2] == result[2]

## Remove RT (retweet)
data.twitter.all <- remove_RT_retweetted(data.twitter.all)

## REmove Links
data.twitter.all <- remove_links(texts =  data.twitter.all)
data.news.all <- remove_links(texts =  data.news.all)
data.blogs.all <- remove_links(texts = data.blogs.all)

## Remove Contractions
data.twitter.all <- remove_contractions(theTexts = data.twitter.all)
data.news.all <- remove_contractions(theTexts = data.news.all)
data.blogs.all <- remove_contractions(theTexts = data.blogs.all)

save(stopwords.profanityWords, data.twitter.all, data.news.all, data.blogs.all, file = "./../data/processed/step1_afterCleaning.Rdata")
```


## Sampling of the Corpora

```{r dataReductionForExploration, collapse=T, echo = F}
#load("./../data/processed/step1_afterCleaning.Rdata")

set.seed(19711004)
coin.biased.outcome <- rbinom(length(data.twitter.all), 1, 0.05)
#table(coin.biased.outcome)
data.twitter.sample <- data.twitter.all[coin.biased.outcome == 1]

set.seed(19711004)
coin.biased.outcome <- rbinom(length(data.news.all), 1, 0.10)
#table(coin.biased.outcome)
data.news.sample <- data.news.all[coin.biased.outcome == 1]

set.seed(19711004)
coin.biased.outcome <- rbinom(length(data.blogs.all), 1, 0.10)
#table(coin.biased.outcome)
data.blogs.sample <- data.blogs.all[coin.biased.outcome == 1]

save(stopwords.profanityWords, data.twitter.sample, data.news.sample, data.blogs.sample, file = "./../data/processed/step2_afterSampling.Rdata")
rm(data.twitter.all, data.news.all, data.blogs.all)
```

For this analysis it is not needed use all of the data. Often relatively few randomly selected rows or chunks need to be included to get an accurate approximation to results that would be obtained using all the data. A "biased coin" approach has been used to select the tweets, news and blogs to be included in the analysis based on the following percentages

* __5%__ of the tweets (__`r length(data.twitter.sample)`__ tweets)
* __10%__ of the news (__`r length(data.news.sample)`__ news)
* __10%__ of the blogs (__`r length(data.blogs.sample)`__ blogs)

# Exploring the (Sample) Corpora

Exploration of the corpora is done using __natural language processing techniques__ - specifically term frequency analysis using ngrams (1-gram, 2-gram and 3-gram). Before tokenizing the corpora the following steps are performed:

* transform to lower case
* remove profanity words
* remove numbers
* remove punctuations - except of the `'` (apostrophe) in order to not lose contractions (e.g. I'll, I'm, etc)
* add a `<s> ` marker at the beginning of each entry (tweet, news, blog)
* add a ` </s>` marker at the end of each entry (tweet, news, blog) 

`Wordclouds` and `barplots` are used to visualize the most frequent words/ tokens for the different n-grams. When plotting the 'barplots' only the first most frequent terms (top 30) are shown and max 200 terms in the wordclouds. __Note:__ For 2-grams and 3-grams a token like `<s> at the` refers to `at the` at the beginning of the entry (tweet, news or blog), while `the top </s>` refers to `the top` at the end of the entry (tweet, news or blog).

```{r libraries, include = F, echo = F}
#load("./../data/processed/step2_afterSampling.Rdata")

require(tm)
require(wordcloud)
require(RWeka)
require(ggplot2)
require(RColorBrewer)

##In order to keep contractions like I'm or I'll ....
removePunctuations.exceptApostrophe <- function(texts){
    gsub(pattern = "[^'[:^punct:]]", replacement = " ", x = texts, perl = T)
}

# test <- "I like %$@to*&, chew;: gum, but don't like|}{[] bubble@#^)( gum!?"
# test.expected <- "I like    to    chew   gum  but don't like      bubble      gum  "
# test.expected == removePunctuations.exceptApostrophe(texts = test)

##Add start sentence (<s>) and end sentence (</s>) markers in the sentences
addStartEndMarkers <- function(texts){
    paste("<s>", texts, "</s>")
}

# test <- c("I love nlp.", "I like the sea.")
# test.expected <- c("<s> I love nlp. </s>", "<s> I like the sea. </s>")
# test.expected == addStartEndMarkers(texts = test)


# tdm.generate.1g <- function(x){
#   corpus <- Corpus(VectorSource(x))
#   corpus <- tm_map(corpus, content_transformer(tolower))
#   corpus <- tm_map(corpus, removeWords, stopwords.profanityWords)
#   corpus <- tm_map(corpus, removeNumbers) 
#   corpus <- tm_map(corpus, content_transformer(removePunctuations.exceptApostrophe))
#   corpus <- tm_map(corpus, stripWhitespace)
#   tdm <- TermDocumentMatrix(corpus)
#   tdm
# }

tdm.generate.ng <- function(x, ng = 1){
  corpus <- Corpus(VectorSource(x))
  corpus <- tm_map(corpus, content_transformer(tolower))
  corpus <- tm_map(corpus, removeWords, stopwords.profanityWords)
  corpus <- tm_map(corpus, removeNumbers) 
  corpus <- tm_map(corpus, content_transformer(removePunctuations.exceptApostrophe))
  corpus <- tm_map(corpus, content_transformer(addStartEndMarkers))
  corpus <- tm_map(corpus, stripWhitespace)
  # MAC OS Manadtory if not using doMC library
  #options(mc.cores=1) 
  ngramTokenizer <- function(y) NGramTokenizer(y, Weka_control(min = ng, max = ng, delimiters = " \\r\\n\\t.,;:\"()?!")) 
  # create n-grams
  tdm <- TermDocumentMatrix(corpus, control = list(tokenize = ngramTokenizer)) # create tdm from n-grams
  tdm
}

getTermFrequencyInformationOrderedByTermFrequency <- function(aTdm, lowFrequency){
    ft.lf <- findFreqTerms(aTdm,lowfreq = lowFrequency)
    aTdm.l <- aTdm[ft.lf,]
    aTdm.l.asMatrix <- as.matrix(aTdm.l)
    # calculate frequency of each term
    aTdm.l.termFreq <- rowSums(aTdm.l.asMatrix)
    # create data frame from subset of terms
    aTdm.l.termFreq.df <- data.frame(term = names(aTdm.l.termFreq), freq = aTdm.l.termFreq)
    # sort by subset DataFrame frequency
    aTdm.l.termFreq.df[with(aTdm.l.termFreq.df, order(-aTdm.l.termFreq.df$freq)), ]
}

visualizeBarPlot <- function(ftm.df, colorBars = "grey40", titleBarPlot = ""){
    
    ggplot(ftm.df[1:30,], aes(x = reorder(term,freq), y = freq/1000)) +
        geom_bar(stat = "identity", fill=colorBars) +
        xlab("Terms") + ylab("Frequency (* 1000)")+
        ggtitle(paste(titleBarPlot, "(Top 30)"))  + coord_flip()

}

visualizeWordcloud <- function(ftm.df){
    mypal <- brewer.pal(8,"Dark2")
    wordcloud(words = ftm.df$term,
          freq = ftm.df$freq, 
          colors = mypal, 
          scale=c(6,.5),
          random.order = F, max.words = 200)
}

visualizeCumuluativeCoverage <- function(allFtm.df, title, filter = NULL){
    idx <- order(allFtm.df$freq, decreasing = T)
    terms.term <- rownames(allFtm.df)[idx]
    terms.freq <- allFtm.df$freq[idx]
    
    if(!is.null(filter)){
        # print(paste("Removing idx", filter))
        terms.term <- terms.term[-filter]
        terms.freq <- terms.freq[-filter]
    }
    
    terms.count <- sum(terms.freq)
    terms.unique <- length(unique(terms.term))
    terms.cumulativeCoverage <- (cumsum(terms.freq)/ terms.count) * 100
    
    
    plot(terms.cumulativeCoverage, type = "l", xlab = "Number Of Words", ylab = "% Coverage", main = title)
    abline(v = which(terms.cumulativeCoverage >= 50)[1], col = "orange", lwd = 3)
    abline(v = which(terms.cumulativeCoverage >= 90)[1], col = "red", lwd = 3)
    legend(x = "topright", lty=c(1,1), lwd=c(3,3), col=c("orange", "red"), legend = c("50% coverage", "90% coverage")) 
}

getSomeInfoABoutCorpora <- function(allFtm.df, title, filter = NULL){
    idx <- order(allFtm.df$freq, decreasing = T)
    terms.term <- rownames(allFtm.df)[idx]
    terms.freq <- allFtm.df$freq[idx]
    
    if(!is.null(filter)){
        # print(paste("Removing idx", filter))
        terms.term <- terms.term[-filter]
        terms.freq <- terms.freq[-filter]
    }
    
    terms.count <- sum(terms.freq)
    terms.unique <- length(unique(terms.term))
    terms.cumulativeCoverage <- (cumsum(terms.freq)/ terms.count) * 100
    coverage.50 <- which(terms.cumulativeCoverage >= 50)[1]
    coverage.90 <- which(terms.cumulativeCoverage >= 90)[1]
    
    list(N = terms.count, V = terms.unique, C50 = coverage.50, C90 = coverage.90)
}
```

```{r generateTermDocumentMatrix, eval = F, cache = T, echo = F}
twitter.corpora.tdm.1g <- tdm.generate.ng(data.twitter.sample)
save(twitter.corpora.tdm.1g, file = "./../data/processed/twitter.tdm.1g.Rdata")
rm(twitter.corpora.tdm.1g)

news.corpora.tdm.1g <- tdm.generate.ng(data.news.sample)
save(news.corpora.tdm.1g, file = "./../data/processed/news.tdm.1g.Rdata")
rm(news.corpora.tdm.1g)

blogs.corpora.tdm.1g <- tdm.generate.ng(data.blogs.sample)
save(blogs.corpora.tdm.1g, file = "./../data/processed/blogs.tdm.1g.Rdata")
rm(blogs.corpora.tdm.1g)

## Bi-grams
twitter.corpora.tdm.2g <- tdm.generate.ng(x = data.twitter.sample,ng = 2)
save(twitter.corpora.tdm.2g, file = "./../data/processed/twitter.tdm.2g.Rdata")
rm(twitter.corpora.tdm.2g)

news.corpora.tdm.2g <- tdm.generate.ng(x = data.news.sample, ng = 2)
save(news.corpora.tdm.2g, file = "./../data/processed/news.tdm.2g.Rdata")
rm(news.corpora.tdm.2g)

blogs.corpora.tdm.2g <- tdm.generate.ng(x = data.blogs.sample, ng = 2)
save(blogs.corpora.tdm.2g, file = "./../data/processed/blogs.tdm.2g.Rdata")
rm(blogs.corpora.tdm.2g)

## Tri-grams
twitter.corpora.tdm.3g <- tdm.generate.ng(x = data.twitter.sample,ng = 3)
save(twitter.corpora.tdm.3g, file = "./../data/processed/twitter.tdm.3g.Rdata")
rm(twitter.corpora.tdm.3g)

news.corpora.tdm.3g <- tdm.generate.ng(x = data.news.sample, ng = 3)
save(news.corpora.tdm.3g, file = "./../data/processed/news.tdm.3g.Rdata")
rm(news.corpora.tdm.3g)

blogs.corpora.tdm.3g <- tdm.generate.ng(x = data.blogs.sample, ng = 3)
save(blogs.corpora.tdm.3g, file = "./../data/processed/blogs.tdm.3g.Rdata")
rm(blogs.corpora.tdm.3g)
```

```{r generateAllTermsFrequency, eval=F, echo = F}
getAllTermsFrequencyInCorpora.as.df <- function(corpora.tdm, chunck = 2000){
    
    print(corpora.tdm)
    
    isFinished <- F
    i <- 0
    
    result <- NULL
    
    while (!isFinished){
        
        i.next <- i + 1
        start <- 1
        end <- chunck
        
        if(i != 0){
            start <- i * chunck + 1
            end <- i.next * chunck
            if((i.next * chunck) > dim(corpora.tdm)[1]){
                end <- dim(corpora.tdm)[1]
                isFinished <- T
            }
        }
        
        range <- start : end
        
        print(paste("Processing Chunck:", start, end))
        x <- corpora.tdm[range,]
        x.asMatrix <- as.matrix(x)
        
        result <- c(result, rowSums(x.asMatrix))
        
        i <- i + 1
    }
    
    allTermsFrequencies.df <- data.frame(freq = result)
}


##Twitter Corpora
load("./../data/processed/twitter.tdm.1g.Rdata")
load("./../data/processed/twitter.tdm.2g.Rdata")
load("./../data/processed/twitter.tdm.3g.Rdata")

corpora.allTermsFrequency <- NULL
corpora.tdm <- twitter.corpora.tdm.1g
corpora.allTermsFrequency <- getAllTermsFrequencyInCorpora.as.df(corpora.tdm = corpora.tdm, chunck = 2000)
save(corpora.allTermsFrequency, file = "./../data/processed/twitter.allTermsFrequency.1g.Rdata")

corpora.allTermsFrequency <- NULL
corpora.tdm <- twitter.corpora.tdm.2g
corpora.allTermsFrequency <- getAllTermsFrequencyInCorpora.as.df(corpora.tdm = corpora.tdm, chunck = 2000)
save(corpora.allTermsFrequency, file = "./../data/processed/twitter.allTermsFrequency.2g.Rdata")

corpora.allTermsFrequency <- NULL
corpora.tdm <- twitter.corpora.tdm.3g
corpora.allTermsFrequency <- getAllTermsFrequencyInCorpora.as.df(corpora.tdm = corpora.tdm, chunck = 2000)
save(corpora.allTermsFrequency, file = "./../data/processed/twitter.allTermsFrequency.3g.Rdata")

corpora.allTermsFrequency <- NULL
corpora.tdm <- NULL
rm(twitter.corpora.tdm.1g, twitter.corpora.tdm.2g, twitter.corpora.tdm.3g)

## NEWS

load("./../data/processed/news.tdm.1g.Rdata")
load("./../data/processed/news.tdm.2g.Rdata")
load("./../data/processed/news.tdm.3g.Rdata")

corpora.allTermsFrequency <- NULL
corpora.tdm <- news.corpora.tdm.1g
corpora.allTermsFrequency <- getAllTermsFrequencyInCorpora.as.df(corpora.tdm = corpora.tdm, chunck = 2000)
save(corpora.allTermsFrequency, file = "./../data/processed/news.allTermsFrequency.1g.Rdata")

corpora.allTermsFrequency <- NULL
corpora.tdm <- news.corpora.tdm.2g
corpora.allTermsFrequency <- getAllTermsFrequencyInCorpora.as.df(corpora.tdm = corpora.tdm, chunck = 2000)
save(corpora.allTermsFrequency, file = "./../data/processed/news.allTermsFrequency.2g.Rdata")

corpora.allTermsFrequency <- NULL
corpora.tdm <- news.corpora.tdm.3g
corpora.allTermsFrequency <- getAllTermsFrequencyInCorpora.as.df(corpora.tdm = corpora.tdm, chunck = 2000)
save(corpora.allTermsFrequency, file = "./../data/processed/news.allTermsFrequency.3g.Rdata")

corpora.allTermsFrequency <- NULL
corpora.tdm <- NULL
rm(news.corpora.tdm.1g, news.corpora.tdm.2g, news.corpora.tdm.3g)

## Blogs

load("./../data/processed/blogs.tdm.1g.Rdata")
load("./../data/processed/blogs.tdm.2g.Rdata")
load("./../data/processed/blogs.tdm.3g.Rdata")

corpora.allTermsFrequency <- NULL
corpora.tdm <- blogs.corpora.tdm.1g
corpora.allTermsFrequency <- getAllTermsFrequencyInCorpora.as.df(corpora.tdm = corpora.tdm, chunck = 2000)
save(corpora.allTermsFrequency, file = "./../data/processed/blogs.allTermsFrequency.1g.Rdata")

corpora.allTermsFrequency <- NULL
corpora.tdm <- blogs.corpora.tdm.2g
corpora.allTermsFrequency <- getAllTermsFrequencyInCorpora.as.df(corpora.tdm = corpora.tdm, chunck = 2000)
save(corpora.allTermsFrequency, file = "./../data/processed/blogs.allTermsFrequency.2g.Rdata")

corpora.allTermsFrequency <- NULL
corpora.tdm <- blogs.corpora.tdm.3g
corpora.allTermsFrequency <- getAllTermsFrequencyInCorpora.as.df(corpora.tdm = corpora.tdm, chunck = 2000)
save(corpora.allTermsFrequency, file = "./../data/processed/blogs.allTermsFrequency.3g.Rdata")

corpora.allTermsFrequency <- NULL
corpora.tdm <- NULL
rm(blogs.corpora.tdm.1g, blogs.corpora.tdm.2g, blogs.corpora.tdm.3g)

```

## Twitter Corpora

```{r loadTwitterData, eval=T, echo = F}
load("./../data/processed/twitter.tdm.1g.Rdata")
load("./../data/processed/twitter.tdm.2g.Rdata")
load("./../data/processed/twitter.tdm.3g.Rdata")

load("./../data/processed/twitter.allTermsFrequency.1g.Rdata")
twitter.allTerms.1g <- corpora.allTermsFrequency

load("./../data/processed/twitter.allTermsFrequency.2g.Rdata")
twitter.allTerms.2g <- corpora.allTermsFrequency

load("./../data/processed/twitter.allTermsFrequency.3g.Rdata")
twitter.allTerms.3g <- corpora.allTermsFrequency
```


### 1-grams

```{r visualizeTwitterData_1g, eval=T, warning = F, message = F, echo = F}
corpora.tdm <- twitter.corpora.tdm.1g

ft.lf.1500 <- findFreqTerms(corpora.tdm,lowfreq = 1500)

frequentTermsLimited.df <- getTermFrequencyInformationOrderedByTermFrequency(corpora.tdm, 1500)

visualizeBarPlot(ftm.df = frequentTermsLimited.df[-c(1,2),], titleBarPlot = "Frequent 1-grams")
visualizeWordcloud(ftm.df = frequentTermsLimited.df[-c(1,2),])
a <- getSomeInfoABoutCorpora(allFtm.df = twitter.allTerms.1g, filter = c(1:2))
```

__How many unique words do you need in a frequency sorted dictionary to cover 50% of all word instances in the language? 90%?__

| N = number of tokens | V = vocabulary size | 50% coverage | 90% coverage |
| ------------- |:-------------:| -----:|-----:|-----:|
| `r format(a$N, digits = 12, nsmall = 0)` | `r a$V` | `r a$C50` | `r a$C90` |


```{r echo=F}
visualizeCumuluativeCoverage(allFtm.df = twitter.allTerms.1g, title = "% Coverage By no of Unique Words (1-grams)", filter = c(1:2))
```

### 2-grams

```{r visualizeTwitterData_2g, eval=T, warning = F, message = F, echo = F}
corpora.tdm <- twitter.corpora.tdm.2g

ft.lf.1000 <- findFreqTerms(corpora.tdm,lowfreq = 1000)

frequentTermsLimited.df <- getTermFrequencyInformationOrderedByTermFrequency(corpora.tdm, 1000)

visualizeBarPlot(ftm.df = frequentTermsLimited.df,titleBarPlot = "Frequent 2-grams")
visualizeWordcloud(ftm.df = frequentTermsLimited.df)
a <- getSomeInfoABoutCorpora(allFtm.df = twitter.allTerms.2g)
```

__How many unique words do you need in a frequency sorted dictionary to cover 50% of all word instances in the language? 90%?__

| N = number of tokens | V = vocabulary size | 50% coverage | 90% coverage |
| ------------- |:-------------:| -----:|-----:|-----:|
| `r format(a$N, digits = 12, nsmall = 0)` | `r a$V` | `r a$C50` | `r a$C90` |


```{r echo=F}
visualizeCumuluativeCoverage(allFtm.df = twitter.allTerms.2g, title = "% Coverage By no of Unique Words (2-grams)")
```

### 3-grams

```{r visualizeTwitterData_3g, eval=T, warning = F, message = F, echo = F}
corpora.tdm <- twitter.corpora.tdm.3g

ft.lf.150 <- findFreqTerms(corpora.tdm,lowfreq = 150)

frequentTermsLimited.df <- getTermFrequencyInformationOrderedByTermFrequency(corpora.tdm, 150)

visualizeBarPlot(ftm.df = frequentTermsLimited.df,titleBarPlot = "Frequent 3-grams")
visualizeWordcloud(ftm.df = frequentTermsLimited.df)
a <- getSomeInfoABoutCorpora(allFtm.df = twitter.allTerms.3g)
```

__How many unique words do you need in a frequency sorted dictionary to cover 50% of all word instances in the language? 90%?__

| N = number of tokens | V = vocabulary size | 50% coverage | 90% coverage |
| ------------- |:-------------:| -----:|-----:|-----:|
| `r format(a$N, digits = 12, nsmall = 0)` | `r a$V` | `r a$C50` | `r a$C90` |


```{r echo=F}
visualizeCumuluativeCoverage(allFtm.df = twitter.allTerms.3g, title = "% Coverage By no of Unique Words (3-grams)")
```

## News Corpora

```{r loadNewsData, eval=T, echo = F}
load("./../data/processed/news.tdm.1g.Rdata")
load("./../data/processed/news.tdm.2g.Rdata")
load("./../data/processed/news.tdm.3g.Rdata")

load("./../data/processed/news.allTermsFrequency.1g.Rdata")
news.allTerms.1g <- corpora.allTermsFrequency

load("./../data/processed/news.allTermsFrequency.2g.Rdata")
news.allTerms.2g <- corpora.allTermsFrequency

load("./../data/processed/news.allTermsFrequency.3g.Rdata")
news.allTerms.3g <- corpora.allTermsFrequency
```

### 1-grams

```{r visualizeNewsData_1g, eval=T, warning = F, message = F, echo = F}
corpora.tdm <- news.corpora.tdm.1g

ft.lf.3500 <- findFreqTerms(corpora.tdm,lowfreq = 3500)

frequentTermsLimited.df <- getTermFrequencyInformationOrderedByTermFrequency(corpora.tdm, 3500)
visualizeBarPlot(ftm.df = frequentTermsLimited.df[-c(2,3),],titleBarPlot = "Frequent 1-grams")
visualizeWordcloud(ftm.df = frequentTermsLimited.df[-c(2,3),])
a <- getSomeInfoABoutCorpora(allFtm.df = news.allTerms.1g, filter = c(2,3))
```

__How many unique words do you need in a frequency sorted dictionary to cover 50% of all word instances in the language? 90%?__

| N = number of tokens | V = vocabulary size | 50% coverage | 90% coverage |
| ------------- |:-------------:| -----:|-----:|-----:|
| `r format(a$N, digits = 12, nsmall = 0)` | `r a$V` | `r a$C50` | `r a$C90` |

```{r echo=F}
visualizeCumuluativeCoverage(allFtm.df = news.allTerms.1g, title = "% Coverage By no of Unique Words (1-grams)", filter = c(2,3))
```

### 2-grams

```{r visualizeNewsData_2g, eval=T, warning = F, message = F, echo = F}
corpora.tdm <- news.corpora.tdm.2g

ft.lf.2000 <- findFreqTerms(corpora.tdm,lowfreq = 2000)

frequentTermsLimited.df <- getTermFrequencyInformationOrderedByTermFrequency(corpora.tdm, 2000)

visualizeBarPlot(ftm.df = frequentTermsLimited.df,titleBarPlot = "Frequent 2-grams")
visualizeWordcloud(ftm.df = frequentTermsLimited.df)
a <- getSomeInfoABoutCorpora(allFtm.df = news.allTerms.2g)
```

__How many unique words do you need in a frequency sorted dictionary to cover 50% of all word instances in the language? 90%?__

| N = number of tokens | V = vocabulary size | 50% coverage | 90% coverage |
| ------------- |:-------------:| -----:|-----:|-----:|
| `r format(a$N, digits = 12, nsmall = 0)` | `r a$V` | `r a$C50` | `r a$C90` |


```{r echo=F}
visualizeCumuluativeCoverage(allFtm.df = news.allTerms.2g, title = "% Coverage By no of Unique Words  (2-grams)")
```

### 3-grams

```{r visualizeNewsData_3g, eval=T, warning = F, message = F, echo = F}
corpora.tdm <- news.corpora.tdm.3g

ft.lf.300 <- findFreqTerms(corpora.tdm,lowfreq = 300)

frequentTermsLimited.df <- getTermFrequencyInformationOrderedByTermFrequency(corpora.tdm, 300)

visualizeBarPlot(ftm.df = frequentTermsLimited.df,titleBarPlot = "Frequent 3-grams")
visualizeWordcloud(ftm.df = frequentTermsLimited.df)
a <- getSomeInfoABoutCorpora(allFtm.df = news.allTerms.3g)
```

__How many unique words do you need in a frequency sorted dictionary to cover 50% of all word instances in the language? 90%?__

| N = number of tokens | V = vocabulary size | 50% coverage | 90% coverage |
| ------------- |:-------------:| -----:|-----:|-----:|
| `r format(a$N, digits = 12, nsmall = 0)` | `r a$V` | `r a$C50` | `r a$C90` |


```{r echo=F}
visualizeCumuluativeCoverage(allFtm.df = news.allTerms.3g, title = "% Coverage By no of Unique Words (3-grams)")
```

## Blogs Corpora

```{r loadBlogsData, eval=T, warning = F, message = F, echo = F}
load("./../data/processed/blogs.tdm.1g.Rdata")
load("./../data/processed/blogs.tdm.2g.Rdata")
load("./../data/processed/blogs.tdm.3g.Rdata")

load("./../data/processed/blogs.allTermsFrequency.1g.Rdata")
blogs.allTerms.1g <- corpora.allTermsFrequency

load("./../data/processed/blogs.allTermsFrequency.2g.Rdata")
blogs.allTerms.2g <- corpora.allTermsFrequency

load("./../data/processed/blogs.allTermsFrequency.3g.Rdata")
blogs.allTerms.3g <- corpora.allTermsFrequency
```

### 1-grams

```{r visualizeBlogsData_1g, eval=T, warning = F, message = F, echo = F}
corpora.tdm <- blogs.corpora.tdm.1g

ft.lf.5000 <- findFreqTerms(corpora.tdm,lowfreq = 5000)

frequentTermsLimited.df <- getTermFrequencyInformationOrderedByTermFrequency(corpora.tdm, 5000)

visualizeBarPlot(ftm.df = frequentTermsLimited.df[-c(3,4),],titleBarPlot = "Frequent 1-grams")
visualizeWordcloud(ftm.df = frequentTermsLimited.df[-c(3,4),])
a <- getSomeInfoABoutCorpora(allFtm.df = blogs.allTerms.1g, filter = c(3,4))
```

__How many unique words do you need in a frequency sorted dictionary to cover 50% of all word instances in the language? 90%?__

| N = number of tokens | V = vocabulary size | 50% coverage | 90% coverage |
| ------------- |:-------------:| -----:|-----:|-----:|
| `r format(a$N, digits = 12, nsmall = 0)` | `r a$V` | `r a$C50` | `r a$C90` |


```{r echo=F}
visualizeCumuluativeCoverage(allFtm.df = blogs.allTerms.1g, title = "% Coverage By no of Unique Words (1-grams)", filter = c(3,4))
```

### 2-grams

```{r visualizeBlogsData_2g, eval=T, warning = F, message = F, echo = F}
corpora.tdm <- blogs.corpora.tdm.2g

ft.lf.2500 <- findFreqTerms(corpora.tdm,lowfreq = 2500)

frequentTermsLimited.df <- getTermFrequencyInformationOrderedByTermFrequency(corpora.tdm, 2500)

visualizeBarPlot(ftm.df = frequentTermsLimited.df,titleBarPlot = "Frequent 2-grams")
visualizeWordcloud(ftm.df = frequentTermsLimited.df)
a <- getSomeInfoABoutCorpora(allFtm.df = blogs.allTerms.2g)
```

__How many unique words do you need in a frequency sorted dictionary to cover 50% of all word instances in the language? 90%?__

| N = number of tokens | V = vocabulary size | 50% coverage | 90% coverage |
| ------------- |:-------------:| -----:|-----:|-----:|
| `r format(a$N, digits = 12, nsmall = 0)` | `r a$V` | `r a$C50` | `r a$C90` |


```{r echo=F}
visualizeCumuluativeCoverage(allFtm.df = blogs.allTerms.2g, title = "% Coverage By no of Unique Words (2-grams)")
```

### 3-grams

```{r visualizeBlogsData_3g, eval=T, warning = F, message = F, echo = F}
corpora.tdm <- blogs.corpora.tdm.3g

ft.lf.300 <- findFreqTerms(corpora.tdm,lowfreq = 300)

frequentTermsLimited.df <- getTermFrequencyInformationOrderedByTermFrequency(corpora.tdm, 300)

visualizeBarPlot(ftm.df = frequentTermsLimited.df,titleBarPlot = "Frequent 3-grams")
visualizeWordcloud(ftm.df = frequentTermsLimited.df)
a <- getSomeInfoABoutCorpora(allFtm.df = blogs.allTerms.3g)
```

__How many unique words do you need in a frequency sorted dictionary to cover 50% of all word instances in the language? 90%?__

| N = number of tokens | V = vocabulary size | 50% coverage | 90% coverage |
| ------------- |:-------------:| -----:|-----:|-----:|
| `r format(a$N, digits = 12, nsmall = 0)` | `r a$V` | `r a$C50` | `r a$C90` |


```{r  echo=F}
visualizeCumuluativeCoverage(allFtm.df = blogs.allTerms.3g, title = "% Coverage By no of Unique Words (3-grams)")
```

# Next Steps

* Investigate the possibility to increase sampling of the different Corpora (twitter, news, blogs)
* Identify a common vocabulary between the different Corpora (twitter, news, blogs)
* Create language models using 1-grams, 2-grams and 3-grams
* Think on how ot use the product/ define a simple GUI
