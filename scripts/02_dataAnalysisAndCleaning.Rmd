---
title: "WIP Exploration and Cleaning"
author: "Pier Lorenzo Paracchini"
date: "25 april 2016"
output: 
  html_document: 
    keep_md: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

getOption("encoding")
Sys.getlocale("LC_CTYPE")
localeToCharset()

# WINDOWS LOCALE SETTING
Sys.setlocale(category = "LC_ALL",locale = "English_United States.1252")

getOption("encoding")
Sys.getlocale("LC_CTYPE")
localeToCharset()

```

## Summary


## Load the data

```{r dataReductionForExploration, collapse=T, cache = T}
con <- file("./../data/original/final/en_US/en_US.twitter.txt", "r") 
data.twitter.all <- readLines(con, skipNul = T)
data.twitter.all.nchar <- nchar(data.twitter.all)
close(con)

con <- file("./../data/original/final/en_US/en_US.news.txt", "r") 
data.news.all <- readLines(con, skipNul = T)
data.news.all.nchar <- nchar(data.news.all)
close(con)

con <- file("./../data/original/final/en_US/en_US.blogs.txt", "r") 
data.blogs.all <- readLines(con, skipNul = T)
data.blogs.all.nchar <- nchar(data.blogs.all)
close(con)

```

```{r someDataStatistics, collapse=T}
data.sources <- c("twitters", "news", "blogs")
data.noOfLines <- c(length(data.twitter.all), length(data.news.all), length(data.blogs.all))
data.maxNoOfChars <- c(max(data.twitter.all.nchar), max(data.news.all.nchar), max(data.blogs.all.nchar))
data.minNoOfChars <- c(min(data.twitter.all.nchar), min(data.news.all.nchar), min(data.blogs.all.nchar))

data.info.df <- data.frame(sources = data.sources, 
                           noOfLines = data.noOfLines, 
                           maxNoOfChar = data.maxNoOfChars,
                           minNoOfChar = data.minNoOfChars)

data.info.df

#######Twitters
hist(nchar(data.twitter.all), main = "No Of Chars per Twitter")

#How many twitter are under 20 chars?
sum(data.twitter.all.nchar <= 20)
twitter.lessThanOr3chars <- data.twitter.all[data.twitter.all.nchar <= 20]
head(twitter.lessThanOr3chars, 20)
tail(twitter.lessThanOr3chars, 20)

#Removing such short tweets
data.twitter.all <- data.twitter.all[data.twitter.all.nchar > 20]
data.twitter.all.nchar <- nchar(data.twitter.all)

twitter.df <- data.frame(text = data.twitter.all, nchar = data.twitter.all.nchar)

##What about twitters more than 140? Analysis

######News
hist(nchar(data.news.all), main = "No Of Chars per News")

#Let's focus on news
a <- data.news.all.nchar < 1500
sum(a)
hist(nchar(data.news.all[a]), main = "No Of Chars per News (< 1500)", breaks = 100)

sum(!a)
hist(nchar(data.news.all[!a]), main = "No Of Chars per News (>= 1500)", breaks = 100)

a <- data.news.all.nchar <= 20
sum(a)
head(data.news.all[a], 40)
tail(data.news.all[a], 40)

#Removing such short news
data.news.all <- data.news.all[data.news.all.nchar > 20]
data.news.all.nchar <- nchar(data.news.all)

news.df <- data.frame(text = data.news.all, nchar = data.news.all.nchar)

######Blogs
hist(nchar(data.blogs.all), main = "Data Blogs No Of Chars per Blogs")

a <- data.blogs.all.nchar < 1300
sum(a)
hist(nchar(data.blogs.all[a]), main = "Data Blogs No Of Chars per Blogs (< 1300)")

sum(!a)
hist(nchar(data.blogs.all[!a]), main = "Data Blogs No Of Chars per Blogs (>= 1300)", breaks = 10000)

a <- data.blogs.all.nchar <= 20
sum(a)
head(data.blogs.all[a], 40)
tail(data.blogs.all[a], 40)

#Removing such short blogss
data.blogs.all <- data.blogs.all[data.blogs.all.nchar > 20]
data.blogs.all.nchar <- nchar(data.blogs.all)

blogs.df <- data.frame(text = data.blogs.all, nchar = data.blogs.all.nchar)

data1.sources <- c("twitters", "news", "blogs")
data1.noOfLines <- c(length(data.twitter.all), length(data.news.all), length(data.blogs.all))
data1.maxNoOfChars <- c(max(data.twitter.all.nchar), max(data.news.all.nchar), max(data.blogs.all.nchar))
data1.minNoOfChars <- c(min(data.twitter.all.nchar), min(data.news.all.nchar), min(data.blogs.all.nchar))

data1.info.df <- data.frame(sources = data1.sources, 
                           noOfLines = data1.noOfLines, 
                           maxNoOfChar = data1.maxNoOfChars,
                           minNoOfChar = data1.minNoOfChars)
data1.info.df
```



### Sampling

```{r dataReductionForExploration, collapse=T, cache=T}
set.seed(19711004)
coin.biased.outcome <- rbinom(length(data.twitter.all), 1, 0.10)
table(coin.biased.outcome)
data.twitter.sample <- data.twitter.all[coin.biased.outcome == 1]

set.seed(19711004)
coin.biased.outcome <- rbinom(length(data.news.all), 1, 0.20)
table(coin.biased.outcome)
data.news.sample <- data.news.all[coin.biased.outcome == 1]

set.seed(19711004)
coin.biased.outcome <- rbinom(length(data.blogs.all), 1, 0.20)
table(coin.biased.outcome)
data.blogs.sample <- data.blogs.all[coin.biased.outcome == 1]

save(data.twitter.sample, data.news.sample, data.blogs.sample, file = "./../data/processed/datasets_sample.Rdata")
```

```{r visualizeData}
load("./../data/processed/datasets_sample.Rdata")

require(tm)
require(wordcloud)
require(RWeka)
require(ggplot2)


removePunctuations.exceptApostrophe <- function(texts){
    gsub(pattern = "[^'[:^punct:]]", replacement = " ", x = texts, perl = T)
}

test <- "I like %$@to*&, chew;: gum, but don't like|}{[] bubble@#^)( gum!?"
test.expected <- "I like    to    chew   gum  but don't like      bubble      gum  "
test.expected == removePunctuations.exceptApostrophe(texts = test)

tdm.generate <- function(x){
  corpus <- Corpus(VectorSource(x))
  corpus <- tm_map(corpus, content_transformer(tolower))
  #corpus <- tm_map(corpus, removeWords, stopwords.badWords)
  corpus <- tm_map(corpus, removeNumbers) 
  corpus <- tm_map(corpus, content_transformer(removePunctuations.exceptApostrophe))
  corpus <- tm_map(corpus, stripWhitespace)
  tdm <- TermDocumentMatrix(corpus)
  tdm
}


tdm.1g <- tdm.generate(data.twitter.sample[1:50000])
tdm.1g

findFreqTerms(tdm.1g,lowfreq = 100)
findFreqTerms(tdm.1g,lowfreq = 300)
findFreqTerms(tdm.1g,lowfreq = 500)

a <- findFreqTerms(tdm.1g,lowfreq = 500)
tdm.1g.l <- tdm.1g[a,]
tdm.1g.l
tdm.1g.l.asMatrix <- as.matrix(tdm.1g.l)

frequentTerms<-findFreqTerms(tdm.1g,lowfreq = 500)
highFreqTerms <- findFreqTerms(tdm.1g, lowfreq = 1000)

# calculate frequency of each term
term.freq <- rowSums(tdm.1g.l.asMatrix)

# picking only a subset
subsetterm.freq <- subset(term.freq, term.freq >= 700)

# create data frame from subset of terms
frequentTermsSubsetDF <- data.frame(term = names(subsetterm.freq), freq = subsetterm.freq)

# create data frame with all terms
frequentTermsDF <- data.frame(term = names(term.freq), freq = term.freq)

# sort by subset DataFrame frequency
frequentTermsSubsetDF <- frequentTermsSubsetDF[with(frequentTermsSubsetDF, order(-frequentTermsSubsetDF$freq)), ]

# sort by complete DataFrame frequency
frequentTermsDF <- frequentTermsDF[with(frequentTermsDF, order(-frequentTermsDF$freq)), ]

# words by frequency from subset data frame
ggplot(frequentTermsSubsetDF, aes(x = reorder(term,freq), y = freq)) + geom_bar(stat = "identity") +xlab("Terms") + ylab("Frequency") + coord_flip()
wordcloud(words = frequentTermsSubsetDF$term,freq = frequentTermsSubsetDF$freq)


tdm.generate.ngrams <- function(x, ng){
  corpus <- Corpus(VectorSource(x))
  corpus <- tm_map(corpus, content_transformer(tolower))
  #corpus <- tm_map(corpus, removeWords, stopwords.badWords)
  corpus <- tm_map(corpus, removeNumbers) 
  corpus <- tm_map(corpus, content_transformer(removePunctuations.exceptApostrophe))
  corpus <- tm_map(corpus, stripWhitespace)
  # MAC OS Manadtory if not using doMC library
  #options(mc.cores=1) 
  ngramTokenizer <- function(y) NGramTokenizer(y, Weka_control(min = ng, max = ng, delimiters = " \\r\\n\\t.,;:\"()?!")) 
  # create n-grams
  tdm <- TermDocumentMatrix(corpus, control = list(tokenize = ngramTokenizer)) # create tdm from n-grams
  tdm
}

tdm.2g <- tdm.generate.ngrams(x = data.twitter.sample[1:50000],ng = 2)
tdm.2g

findFreqTerms(tdm.2g,lowfreq = 100)
findFreqTerms(tdm.2g,lowfreq = 200)
findFreqTerms(tdm.2g,lowfreq = 300)
findFreqTerms(tdm.2g,lowfreq = 500)

b <- findFreqTerms(tdm.2g,lowfreq = 300)
tdm.2g.l <- tdm.2g[b,]
tdm.2g.l
tdm.2g.l.asMatrix <- as.matrix(tdm.2g.l)

term.2g.freq <- rowSums(tdm.2g.l.asMatrix)
frequentTermsDF.2g <- data.frame(term = names(term.2g.freq), freq = term.2g.freq)

frequentTermsDF.2g <- frequentTermsDF.2g[with(frequentTermsDF.2g, order(-frequentTermsDF.2g$freq)), ]

# words by frequency from subset data frame
ggplot(frequentTermsDF.2g, aes(x = reorder(term,freq), y = freq)) + geom_bar(stat = "identity") +xlab("Terms") + ylab("Frequency") + coord_flip()
wordcloud(words = frequentTermsDF.2g$term,freq = frequentTermsDF.2g$freq)


tdm.3g <- tdm.generate.ngrams(x = data.twitter.sample[1:50000],ng = 3)
tdm.3g

findFreqTerms(tdm.3g,lowfreq = 30)
findFreqTerms(tdm.3g,lowfreq = 40)
findFreqTerms(tdm.3g,lowfreq = 50)
findFreqTerms(tdm.3g,lowfreq = 60)

c <- findFreqTerms(tdm.3g,lowfreq = 50)
tdm.3g.l <- tdm.3g[c,]
tdm.3g.l
tdm.3g.l.asMatrix <- as.matrix(tdm.3g.l)

term.3g.freq <- rowSums(tdm.3g.l.asMatrix)
frequentTermsDF.3g <- data.frame(term = names(term.3g.freq), freq = term.3g.freq)

frequentTermsDF.3g <- frequentTermsDF.3g[with(frequentTermsDF.3g, order(-frequentTermsDF.3g$freq)), ]

# words by frequency from subset data frame
ggplot(frequentTermsDF.3g, aes(x = reorder(term,freq), y = freq)) + geom_bar(stat = "identity") +xlab("Terms") + ylab("Frequency") + coord_flip()
wordcloud(words = frequentTermsDF.3g$term,freq = frequentTermsDF.3g$freq)
```

