---
title: "Twitter Cleaning"
author: "Pier Lorenzo Paracchini"
date: "3 mai 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

#Required Libraries
require(knitr)

# WINDOWS LOCALE SETTING
Sys.setlocale(category = "LC_ALL",locale = "English_United States.1252")
Sys.getlocale("LC_CTYPE")
localeToCharset()
```

```{r dataLoadTwitter, echo = F, cache = T, eval = F}
rm(list = ls())
con <- file("./../data/original/final/en_US/en_US.twitter.txt", "r") 
data.twitter.all <- readLines(con, skipNul = T)
close(con)

data.twitter.all.nchar <- nchar(data.twitter.all)

data.twitter.all[15]
data.twitter.all[10967]
data.twitter.all[7046]

##Using iconv to replace non ASCII char with an empty char
data.twitter.all.ascii <-  iconv(data.twitter.all, from = localeToCharset(), to = "ASCII", "")


##Reset working data after removing Gremlings
data.twitter.all <- data.twitter.all.ascii
data.twitter.all.nchar <- nchar(data.twitter.all)

data.twitter.all[15]
data.twitter.all[10967]
data.twitter.all[7046]

hist(nchar(data.twitter.all), main = "No Of Chars Distribution - Tweets", xlab = "no of characters")

shortTweets.maxNoOfChar <- 20
#Find tweets that are less than 
twitter.lessThanOr3chars <- data.twitter.all[data.twitter.all.nchar <= shortTweets.maxNoOfChar]
twitter.lessThanOr3chars[100:110]

#Removing such short tweets
data.twitter.all <- data.twitter.all[data.twitter.all.nchar > 20]
data.twitter.all.nchar <- nchar(data.twitter.all)

data.twitter.all[15]
data.twitter.all[10967]
data.twitter.all[7046]


replaceContraction <- function(texts, contraction, replaceWith, ignoreCase = F){
    gsub(pattern = contraction, replacement = replaceWith, x = texts, ignore.case = ignoreCase)
}

remove_RT_retweetted <- function(texts){
    gsub(pattern = "RT", replacement = " ", x = texts, ignore.case = F)
}

## Need to be tested
# remove_links <- function(texts, ignoreCase = T){
#     gsub(pattern = "^(https?:\\/\\/)?([\\da-z\\.-]+)\\.([a-z\\.]{2,6})([\\/\\w \\.-]*)*\\/?$", 
#          replacement = " ", x = texts, ignore.case = ignoreCase)
# }


remove_contractions <- function(theTexts){
    rem.contr.tmp <- replaceContraction(texts = theTexts, contraction = " u ", replaceWith = " you ", ignoreCase = T)
    rem.contr.tmp <- replaceContraction(texts = theTexts, contraction = " r ", replaceWith = " are ", ignoreCase = T)
    rem.contr.tmp <- replaceContraction(texts = theTexts, contraction = "c'mon", replaceWith = "come on",ignoreCase = T)
    rem.contr.tmp <- replaceContraction(texts = theTexts, contraction = "doin'", replaceWith = "doing",ignoreCase = T)
    rem.contr.tmp <- replaceContraction(texts = theTexts, contraction = "[yY]a?'a?ll", replaceWith = "you all",ignoreCase = T)
    rem.contr.tmp <- replaceContraction(texts = theTexts, contraction = "ma'am", replaceWith = "madam",ignoreCase = T)
    rem.contr.tmp
}

# test.u <- "I'm coo... Jus at work hella tired are u ever in cali"
# test.u.expected <- "I'm coo... Jus at work hella tired are you ever in cali"
# test.u.expected == replaceContraction(texts = test.u, contraction = " u ", replaceWith = " you ", ignoreCase = T)
# 
# test.r <- "I'm coo... Jus at work hella tired r you ever in cali"
# test.r.expected <- "I'm coo... Jus at work hella tired are you ever in cali"
# test.r.expected == replaceContraction(texts = test.r, contraction = " r ", replaceWith = " are ", ignoreCase = T)
# 
# test.RT <- c("I'm cool... RT : Just at work RT tired r you ever in cali", "RT I'm cool...")
# test.RT.expected <- c("I'm cool...   : Just at work   tired r you ever in cali", "  I'm cool...")
# result <- remove_RT_retweetted(test.RT)
# test.RT.expected[1] == result[1]
# test.RT.expected[2] == result[2]

## Remove RT (retweet)
data.twitter.all <- remove_RT_retweetted(data.twitter.all)

## REmove Links (to be tested)
#data.twitter.all <- remove_links(texts =  data.twitter.all)

## Remove Contractions
data.twitter.all <- remove_contractions(theTexts = data.twitter.all)

data.twitter.all.df <- data.frame(data.twitter.all)
save(data.twitter.all, data.twitter.all.df, file = "./tmp/twitter_step1_all.Rdata")
```


```{r dataReductionForTwitter, collapse=T, echo = T, eval = F}
rm(list = ls())
load(file = "./tmp/twitter_step1_all.Rdata")

set.seed(19711004)
coin.biased.outcome <- rbinom(length(data.twitter.all), 1, 0.2)
#table(coin.biased.outcome)
data.twitter.sample <- data.twitter.all[coin.biased.outcome == 1]

save(data.twitter.sample, file = "./tmp/twitter_step2_sample.Rdata")
rm(list = ls())
```

# Exploring the (Sample) Corpora

Exploration of the corpora is done using __natural language processing techniques__ - specifically term frequency analysis using ngrams (1-gram, 2-gram and 3-gram). Before tokenizing the corpora the following steps are performed:

* transform to lower case
* remove profanity words
* remove numbers
* remove punctuations - except of the `'` (apostrophe) in order to not lose contractions (e.g. I'll, I'm, etc)
* add a `<s> ` marker at the beginning of each entry (tweet, news, blog)
* add a ` </s>` marker at the end of each entry (tweet, news, blog) 

`Wordclouds` and `barplots` are used to visualize the most frequent words/ tokens for the different n-grams. When plotting the 'barplots' only the first most frequent terms (top 30) are shown and max 200 terms in the wordclouds. __Note:__ For 2-grams and 3-grams a token like `<s> at the` refers to `at the` at the beginning of the entry (tweet, news or blog), while `the top </s>` refers to `the top` at the end of the entry (tweet, news or blog).

```{r libraries, include = T, echo = F}
rm(list = ls())
load(file = "./tmp/twitter_step2_sample.Rdata")

require(tm)
require(wordcloud)
require(RWeka)
require(ggplot2)
require(RColorBrewer)

##In order to keep contractions like I'm or I'll ....
removePunctuations.exceptApostrophe <- function(texts){
    gsub(pattern = "[^'[:^punct:]]", replacement = " ", x = texts, perl = T)
}

test <- "I'm I'll I like %$@to*&, chew;: gum, but don't like|}{[] bubble@#^)( gum!?"
test.expected <- "I'm I'll I like    to    chew   gum  but don't like      bubble      gum  "
result <- removePunctuations.exceptApostrophe(texts = test)
test.expected == result
result


##Add start sentence (<s>) and end sentence (</s>) markers in the sentences
addStartEndMarkers <- function(texts){
    paste("<s>", texts, "</s>")
}

test <- c("I love nlp.", "I like the sea.")
test.expected <- c("<s> I love nlp. </s>", "<s> I like the sea. </s>")
result <- addStartEndMarkers(texts = test)
test.expected == result
result

## Removal of Profanity Words
con <- file("./../data/original/bad-words.txt", "r") 
stopwords.profanityWords <- readLines(con, skipNul = T)
close(con)


corpora.transform <- function(x){
  corpus <- Corpus(VectorSource(x))
  corpus <- tm_map(corpus, content_transformer(tolower))
  corpus <- tm_map(corpus, removeWords, stopwords.profanityWords)
  corpus <- tm_map(corpus, removeNumbers) 
  corpus <- tm_map(corpus, content_transformer(removePunctuations.exceptApostrophe))
  corpus <- tm_map(corpus, content_transformer(addStartEndMarkers))
  corpus <- tm_map(corpus, stripWhitespace)
  corpus
}

tdm.generate.ng <- function(corpus, ng = 1){
  # MAC OS Manadtory if not using doMC library
  #options(mc.cores=1) 
  ngramTokenizer <- function(y) NGramTokenizer(y, Weka_control(min = ng, max = ng, delimiters = " \\r\\n\\t.,;:\"()?!")) 
  # create n-grams
  tdm <- TermDocumentMatrix(corpus, control = list(tokenize = ngramTokenizer)) # create tdm from n-grams
  tdm
}

```

```{r generateTermDocumentMatrixAnd SummaryHeavyProcessing, eval = F, cache = T, echo = F}
#Unigrams
twitter.corpora.transformed <- corpora.transform(data.twitter.sample)
lapply(twitter.corpora.transformed[1:5], as.character)
save(twitter.corpora.transformed, file = "./tmp/twitter.corpora.transformed.Rdata")

twitter.corpora.tdm.1g <- tdm.generate.ng(twitter.corpora.transformed)
lapply(twitter.corpora.transformed[1:5], as.character)
save(twitter.corpora.tdm.1g, file = "./tmp/twitter.tdm.1g.Rdata")
rm(twitter.corpora.tdm.1g)


## Bi-grams
twitter.corpora.tdm.2g <- tdm.generate.ng(twitter.corpora.transformed,ng = 2)
lapply(twitter.corpora.transformed[1:5], as.character)
save(twitter.corpora.tdm.2g, file = "./tmp/twitter.tdm.2g.Rdata")
rm(twitter.corpora.tdm.2g)

## Tri-grams
twitter.corpora.tdm.3g <- tdm.generate.ng(twitter.corpora.transformed,ng = 3)
lapply(twitter.corpora.transformed[1:5], as.character)
save(twitter.corpora.tdm.3g, file = "./tmp/twitter.tdm.3g.Rdata")
rm(twitter.corpora.tdm.3g)

rm(list = ls())

getAllTermsFrequencyInCorpora.as.df <- function(corpora.tdm, chunck = 2000){
    
    print(corpora.tdm)

    isFinished <- F
    i <- 0
    
    result <- NULL
    
    while (!isFinished){
        
        i.next <- i + 1
        start <- 1
        end <- chunck
        
        if(i != 0){
            start <- i * chunck + 1
            end <- i.next * chunck
            if((i.next * chunck) > dim(corpora.tdm)[1]){
                end <- dim(corpora.tdm)[1]
                isFinished <- T
            }
        }
        
        range <- start : end
        
        print(paste("Processing Chunck:", start, end))
        x <- corpora.tdm[range,]
        x.asMatrix <- as.matrix(x)
        
        result <- c(result, rowSums(x.asMatrix))
        
        i <- i + 1
    }
    
    allTermsFrequencies.df <- data.frame(freq = result)
}


##Twitter Corpora
load("./tmp/twitter.tdm.1g.Rdata")
corpora.allTermsFrequency <- NULL
corpora.tdm <- twitter.corpora.tdm.1g
corpora.allTermsFrequency <- getAllTermsFrequencyInCorpora.as.df(corpora.tdm = corpora.tdm, chunck = 2000)
save(corpora.allTermsFrequency, file = "./tmp/twitter.allTermsFrequency.1g.Rdata")
rm(twitter.corpora.tdm.1g)

load("./tmp/twitter.tdm.2g.Rdata")
corpora.allTermsFrequency <- NULL
corpora.tdm <- twitter.corpora.tdm.2g
corpora.allTermsFrequency <- getAllTermsFrequencyInCorpora.as.df(corpora.tdm = corpora.tdm, chunck = 2000)
save(corpora.allTermsFrequency, file = "./tmp/twitter.allTermsFrequency.2g.Rdata")
rm(twitter.corpora.tdm.2g)

load("./tmp/twitter.tdm.3g.Rdata")
corpora.allTermsFrequency <- NULL
corpora.tdm <- twitter.corpora.tdm.3g
corpora.allTermsFrequency <- getAllTermsFrequencyInCorpora.as.df(corpora.tdm = corpora.tdm, chunck = 2000)
save(corpora.allTermsFrequency, file = "./tmp/twitter.allTermsFrequency.3g.Rdata")
rm(twitter.corpora.tdm.3g)

corpora.allTermsFrequency <- NULL
corpora.tdm <- NULL

```

## Twitter Corpora

```{r loadTwitterData, eval=T, echo = T}
rm(list = ls())
load("./tmp/twitter.tdm.1g.Rdata")
load("./tmp/twitter.tdm.2g.Rdata")
load("./tmp/twitter.tdm.3g.Rdata")

load("./tmp/twitter.allTermsFrequency.1g.Rdata")
twitter.allTerms.1g <- corpora.allTermsFrequency

load("./tmp/twitter.allTermsFrequency.2g.Rdata")
twitter.allTerms.2g <- corpora.allTermsFrequency

load("./tmp/twitter.allTermsFrequency.3g.Rdata")
twitter.allTerms.3g <- corpora.allTermsFrequency
```


### 1-grams

```{r visualizeTwitterData_1g, eval=T, warning = F, message = F, echo = F}
getTermFrequencyInformationOrderedByTermFrequency <- function(aTdm, lowFrequency){
    ft.lf <- findFreqTerms(aTdm,lowfreq = lowFrequency)
    aTdm.l <- aTdm[ft.lf,]
    aTdm.l.asMatrix <- as.matrix(aTdm.l)
    # calculate frequency of each term
    aTdm.l.termFreq <- rowSums(aTdm.l.asMatrix)
    # create data frame from subset of terms
    aTdm.l.termFreq.df <- data.frame(term = names(aTdm.l.termFreq), freq = aTdm.l.termFreq)
    # sort by subset DataFrame frequency
    aTdm.l.termFreq.df[with(aTdm.l.termFreq.df, order(-aTdm.l.termFreq.df$freq)), ]
}

visualizeBarPlot <- function(ftm.df, colorBars = "grey40", titleBarPlot = ""){
    
    ggplot(ftm.df[1:30,], aes(x = reorder(term,freq), y = freq/1000)) +
        geom_bar(stat = "identity", fill=colorBars) +
        xlab("Terms") + ylab("Frequency (* 1000)")+
        ggtitle(paste(titleBarPlot, "(Top 30)"))  + coord_flip()

}

visualizeWordcloud <- function(ftm.df){
    mypal <- brewer.pal(8,"Dark2")
    wordcloud(words = ftm.df$term,
          freq = ftm.df$freq, 
          colors = mypal, 
          scale=c(6,.5),
          random.order = F, max.words = 200)
}

visualizeCumuluativeCoverage <- function(allFtm.df, title, filter = NULL){
    idx <- order(allFtm.df$freq, decreasing = T)
    terms.term <- rownames(allFtm.df)[idx]
    terms.freq <- allFtm.df$freq[idx]
    
    if(!is.null(filter)){
        # print(paste("Removing idx", filter))
        terms.term <- terms.term[-filter]
        terms.freq <- terms.freq[-filter]
    }
    
    terms.count <- sum(terms.freq)
    terms.unique <- length(unique(terms.term))
    terms.cumulativeCoverage <- (cumsum(terms.freq)/ terms.count) * 100
    
    
    plot(terms.cumulativeCoverage, type = "l", xlab = "Number Of Words", ylab = "% Coverage", main = title)
    abline(v = which(terms.cumulativeCoverage >= 50)[1], col = "orange", lwd = 3)
    abline(v = which(terms.cumulativeCoverage >= 90)[1], col = "red", lwd = 3)
    legend(x = "topright", lty=c(1,1), lwd=c(3,3), col=c("orange", "red"), legend = c("50% coverage", "90% coverage")) 
}

getSomeInfoABoutCorpora <- function(allFtm.df, title, filter = NULL){
    idx <- order(allFtm.df$freq, decreasing = T)
    terms.term <- rownames(allFtm.df)[idx]
    terms.freq <- allFtm.df$freq[idx]
    
    if(!is.null(filter)){
        # print(paste("Removing idx", filter))
        terms.term <- terms.term[-filter]
        terms.freq <- terms.freq[-filter]
    }
    
    terms.count <- sum(terms.freq)
    terms.unique <- length(unique(terms.term))
    terms.cumulativeCoverage <- (cumsum(terms.freq)/ terms.count) * 100
    coverage.50 <- which(terms.cumulativeCoverage >= 50)[1]
    coverage.90 <- which(terms.cumulativeCoverage >= 90)[1]
    
    list(N = terms.count, V = terms.unique, C50 = coverage.50, C90 = coverage.90)
}


corpora.tdm <- twitter.corpora.tdm.1g

ft.lf.1500 <- findFreqTerms(corpora.tdm,lowfreq = 1500)

frequentTermsLimited.df <- getTermFrequencyInformationOrderedByTermFrequency(corpora.tdm, 1500)

visualizeBarPlot(ftm.df = frequentTermsLimited.df[-c(1,2),], titleBarPlot = "Frequent 1-grams")
visualizeWordcloud(ftm.df = frequentTermsLimited.df[-c(1,2),])
a <- getSomeInfoABoutCorpora(allFtm.df = twitter.allTerms.1g, filter = c(1:2))
```

__How many unique words do you need in a frequency sorted dictionary to cover 50% of all word instances in the language? 90%?__

| N = number of tokens | V = vocabulary size | 50% coverage | 90% coverage |
| ------------- |:-------------:| -----:|-----:|-----:|
| `r format(a$N, digits = 12, nsmall = 0)` | `r a$V` | `r a$C50` | `r a$C90` |


```{r echo=F}
visualizeCumuluativeCoverage(allFtm.df = twitter.allTerms.1g, title = "% Coverage By no of Unique Words (1-grams)", filter = c(1:2))
```

### 2-grams

```{r visualizeTwitterData_2g, eval=T, warning = F, message = F, echo = F}
corpora.tdm <- twitter.corpora.tdm.2g

ft.lf.1000 <- findFreqTerms(corpora.tdm,lowfreq = 1000)

frequentTermsLimited.df <- getTermFrequencyInformationOrderedByTermFrequency(corpora.tdm, 1000)

visualizeBarPlot(ftm.df = frequentTermsLimited.df,titleBarPlot = "Frequent 2-grams")
visualizeWordcloud(ftm.df = frequentTermsLimited.df)
a <- getSomeInfoABoutCorpora(allFtm.df = twitter.allTerms.2g)
```

__How many unique words do you need in a frequency sorted dictionary to cover 50% of all word instances in the language? 90%?__

| N = number of tokens | V = vocabulary size | 50% coverage | 90% coverage |
| ------------- |:-------------:| -----:|-----:|-----:|
| `r format(a$N, digits = 12, nsmall = 0)` | `r a$V` | `r a$C50` | `r a$C90` |


```{r echo=F}
visualizeCumuluativeCoverage(allFtm.df = twitter.allTerms.2g, title = "% Coverage By no of Unique Words (2-grams)")
```

### 3-grams

```{r visualizeTwitterData_3g, eval=T, warning = F, message = F, echo = F}
corpora.tdm <- twitter.corpora.tdm.3g

ft.lf.150 <- findFreqTerms(corpora.tdm,lowfreq = 150)

frequentTermsLimited.df <- getTermFrequencyInformationOrderedByTermFrequency(corpora.tdm, 150)

visualizeBarPlot(ftm.df = frequentTermsLimited.df,titleBarPlot = "Frequent 3-grams")
visualizeWordcloud(ftm.df = frequentTermsLimited.df)
a <- getSomeInfoABoutCorpora(allFtm.df = twitter.allTerms.3g)
```

__How many unique words do you need in a frequency sorted dictionary to cover 50% of all word instances in the language? 90%?__

| N = number of tokens | V = vocabulary size | 50% coverage | 90% coverage |
| ------------- |:-------------:| -----:|-----:|-----:|
| `r format(a$N, digits = 12, nsmall = 0)` | `r a$V` | `r a$C50` | `r a$C90` |


```{r echo=F}
visualizeCumuluativeCoverage(allFtm.df = twitter.allTerms.3g, title = "% Coverage By no of Unique Words (3-grams)")
```

## News Corpora

```{r loadNewsData, eval=T, echo = F}
load("./../data/processed/news.tdm.1g.Rdata")
load("./../data/processed/news.tdm.2g.Rdata")
load("./../data/processed/news.tdm.3g.Rdata")

load("./../data/processed/news.allTermsFrequency.1g.Rdata")
news.allTerms.1g <- corpora.allTermsFrequency

load("./../data/processed/news.allTermsFrequency.2g.Rdata")
news.allTerms.2g <- corpora.allTermsFrequency

load("./../data/processed/news.allTermsFrequency.3g.Rdata")
news.allTerms.3g <- corpora.allTermsFrequency
```

### 1-grams

```{r visualizeNewsData_1g, eval=T, warning = F, message = F, echo = F}
corpora.tdm <- news.corpora.tdm.1g

ft.lf.3500 <- findFreqTerms(corpora.tdm,lowfreq = 3500)

frequentTermsLimited.df <- getTermFrequencyInformationOrderedByTermFrequency(corpora.tdm, 3500)
visualizeBarPlot(ftm.df = frequentTermsLimited.df[-c(2,3),],titleBarPlot = "Frequent 1-grams")
visualizeWordcloud(ftm.df = frequentTermsLimited.df[-c(2,3),])
a <- getSomeInfoABoutCorpora(allFtm.df = news.allTerms.1g, filter = c(2,3))
```

__How many unique words do you need in a frequency sorted dictionary to cover 50% of all word instances in the language? 90%?__

| N = number of tokens | V = vocabulary size | 50% coverage | 90% coverage |
| ------------- |:-------------:| -----:|-----:|-----:|
| `r format(a$N, digits = 12, nsmall = 0)` | `r a$V` | `r a$C50` | `r a$C90` |

```{r echo=F}
visualizeCumuluativeCoverage(allFtm.df = news.allTerms.1g, title = "% Coverage By no of Unique Words (1-grams)", filter = c(2,3))
```

### 2-grams

```{r visualizeNewsData_2g, eval=T, warning = F, message = F, echo = F}
corpora.tdm <- news.corpora.tdm.2g

ft.lf.2000 <- findFreqTerms(corpora.tdm,lowfreq = 2000)

frequentTermsLimited.df <- getTermFrequencyInformationOrderedByTermFrequency(corpora.tdm, 2000)

visualizeBarPlot(ftm.df = frequentTermsLimited.df,titleBarPlot = "Frequent 2-grams")
visualizeWordcloud(ftm.df = frequentTermsLimited.df)
a <- getSomeInfoABoutCorpora(allFtm.df = news.allTerms.2g)
```

__How many unique words do you need in a frequency sorted dictionary to cover 50% of all word instances in the language? 90%?__

| N = number of tokens | V = vocabulary size | 50% coverage | 90% coverage |
| ------------- |:-------------:| -----:|-----:|-----:|
| `r format(a$N, digits = 12, nsmall = 0)` | `r a$V` | `r a$C50` | `r a$C90` |


```{r echo=F}
visualizeCumuluativeCoverage(allFtm.df = news.allTerms.2g, title = "% Coverage By no of Unique Words  (2-grams)")
```

### 3-grams

```{r visualizeNewsData_3g, eval=T, warning = F, message = F, echo = F}
corpora.tdm <- news.corpora.tdm.3g

ft.lf.300 <- findFreqTerms(corpora.tdm,lowfreq = 300)

frequentTermsLimited.df <- getTermFrequencyInformationOrderedByTermFrequency(corpora.tdm, 300)

visualizeBarPlot(ftm.df = frequentTermsLimited.df,titleBarPlot = "Frequent 3-grams")
visualizeWordcloud(ftm.df = frequentTermsLimited.df)
a <- getSomeInfoABoutCorpora(allFtm.df = news.allTerms.3g)
```

__How many unique words do you need in a frequency sorted dictionary to cover 50% of all word instances in the language? 90%?__

| N = number of tokens | V = vocabulary size | 50% coverage | 90% coverage |
| ------------- |:-------------:| -----:|-----:|-----:|
| `r format(a$N, digits = 12, nsmall = 0)` | `r a$V` | `r a$C50` | `r a$C90` |


```{r echo=F}
visualizeCumuluativeCoverage(allFtm.df = news.allTerms.3g, title = "% Coverage By no of Unique Words (3-grams)")
```

## Blogs Corpora

```{r loadBlogsData, eval=T, warning = F, message = F, echo = F}
load("./../data/processed/blogs.tdm.1g.Rdata")
load("./../data/processed/blogs.tdm.2g.Rdata")
load("./../data/processed/blogs.tdm.3g.Rdata")

load("./../data/processed/blogs.allTermsFrequency.1g.Rdata")
blogs.allTerms.1g <- corpora.allTermsFrequency

load("./../data/processed/blogs.allTermsFrequency.2g.Rdata")
blogs.allTerms.2g <- corpora.allTermsFrequency

load("./../data/processed/blogs.allTermsFrequency.3g.Rdata")
blogs.allTerms.3g <- corpora.allTermsFrequency
```

### 1-grams

```{r visualizeBlogsData_1g, eval=T, warning = F, message = F, echo = F}
corpora.tdm <- blogs.corpora.tdm.1g

ft.lf.5000 <- findFreqTerms(corpora.tdm,lowfreq = 5000)

frequentTermsLimited.df <- getTermFrequencyInformationOrderedByTermFrequency(corpora.tdm, 5000)

visualizeBarPlot(ftm.df = frequentTermsLimited.df[-c(3,4),],titleBarPlot = "Frequent 1-grams")
visualizeWordcloud(ftm.df = frequentTermsLimited.df[-c(3,4),])
a <- getSomeInfoABoutCorpora(allFtm.df = blogs.allTerms.1g, filter = c(3,4))
```

__How many unique words do you need in a frequency sorted dictionary to cover 50% of all word instances in the language? 90%?__

| N = number of tokens | V = vocabulary size | 50% coverage | 90% coverage |
| ------------- |:-------------:| -----:|-----:|-----:|
| `r format(a$N, digits = 12, nsmall = 0)` | `r a$V` | `r a$C50` | `r a$C90` |


```{r echo=F}
visualizeCumuluativeCoverage(allFtm.df = blogs.allTerms.1g, title = "% Coverage By no of Unique Words (1-grams)", filter = c(3,4))
```

### 2-grams

```{r visualizeBlogsData_2g, eval=T, warning = F, message = F, echo = F}
corpora.tdm <- blogs.corpora.tdm.2g

ft.lf.2500 <- findFreqTerms(corpora.tdm,lowfreq = 2500)

frequentTermsLimited.df <- getTermFrequencyInformationOrderedByTermFrequency(corpora.tdm, 2500)

visualizeBarPlot(ftm.df = frequentTermsLimited.df,titleBarPlot = "Frequent 2-grams")
visualizeWordcloud(ftm.df = frequentTermsLimited.df)
a <- getSomeInfoABoutCorpora(allFtm.df = blogs.allTerms.2g)
```

__How many unique words do you need in a frequency sorted dictionary to cover 50% of all word instances in the language? 90%?__

| N = number of tokens | V = vocabulary size | 50% coverage | 90% coverage |
| ------------- |:-------------:| -----:|-----:|-----:|
| `r format(a$N, digits = 12, nsmall = 0)` | `r a$V` | `r a$C50` | `r a$C90` |


```{r echo=F}
visualizeCumuluativeCoverage(allFtm.df = blogs.allTerms.2g, title = "% Coverage By no of Unique Words (2-grams)")
```

### 3-grams

```{r visualizeBlogsData_3g, eval=T, warning = F, message = F, echo = F}
corpora.tdm <- blogs.corpora.tdm.3g

ft.lf.300 <- findFreqTerms(corpora.tdm,lowfreq = 300)

frequentTermsLimited.df <- getTermFrequencyInformationOrderedByTermFrequency(corpora.tdm, 300)

visualizeBarPlot(ftm.df = frequentTermsLimited.df,titleBarPlot = "Frequent 3-grams")
visualizeWordcloud(ftm.df = frequentTermsLimited.df)
a <- getSomeInfoABoutCorpora(allFtm.df = blogs.allTerms.3g)
```

__How many unique words do you need in a frequency sorted dictionary to cover 50% of all word instances in the language? 90%?__

| N = number of tokens | V = vocabulary size | 50% coverage | 90% coverage |
| ------------- |:-------------:| -----:|-----:|-----:|
| `r format(a$N, digits = 12, nsmall = 0)` | `r a$V` | `r a$C50` | `r a$C90` |


```{r  echo=F}
visualizeCumuluativeCoverage(allFtm.df = blogs.allTerms.3g, title = "% Coverage By no of Unique Words (3-grams)")
```

# Next Steps

* Investigate the possibility to increase sampling of the different Corpora (twitter, news, blogs)
* Identify a common vocabulary between the different Corpora (twitter, news, blogs)
* Create language models using 1-grams, 2-grams and 3-grams
* Think on how ot use the product/ define a simple GUI
