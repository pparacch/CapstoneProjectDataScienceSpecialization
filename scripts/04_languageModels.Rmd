---
title: "Language Models"
author: "Pier Lorenzo Paracchini"
date: "3 mai 2016"
output: 
  html_document: 
    keep_md: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# WINDOWS LOCALE SETTING
Sys.setlocale(category = "LC_ALL",locale = "English_United States.1252")

require(profr)
require(hash)
```

# Language Model Creation - some Notes
AN idea
Optimized Version ... previousTerm, nextTem, count, probability
#Trigrams done
#Bigrams -> to be refactored
#Unigrams -> to be refactored

## Unigram Language Model

```{r unigramModelCreate, collapse = T, eval = F}
## TO BE REVIEWED & REFACTORED
rm(list = ls())
source("./model.R")

tw.1g.list <- load.twitter.1g.data("./../data/processed/")

tw.1g.terms <- rownames(tw.1g.list$allTermsCounters)
tw.1g.counters <- tw.1g.list$allTermsCounters$freq
tw.1g.N <- sum(tw.1g.counters)
tw.1g.V <- length(unique(tw.1g.terms))

#Order by frequency (decreasing)
idx <- order(tw.1g.counters, decreasing = T)
tw.1g.counters <- tw.1g.counters[idx]
tw.1g.terms <- tw.1g.terms[idx]
tw.1g.cumulativeCoverage <- (cumsum(tw.1g.counters)/ tw.1g.N) * 100
idx <- NULL

idx <- which(tw.1g.counters > 1)
length(idx) #25997 words 

plot(tw.1g.cumulativeCoverage)
idx <- which(tw.1g.cumulativeCoverage <= 90)
length(idx) #5248 words 

##Simplification: consider the words in order to have a max cumulative coverage of 
# idx <- which(tw.1g.cumulativeCoverage <= 99.5)
# tw.1g.counters <- tw.1g.counters[idx]
# tw.1g.terms <- tw.1g.terms[idx]
# tw.1g.cumulativeCoverage <- (cumsum(tw.1g.counters)/ tw.1g.N) * 100
# plot(tw.1g.cumulativeCoverage)


##Create the twitter unigram model
Rprof(tmp <- tempfile())
tw.1g.model <- unigrams.model(u.words = tw.1g.terms, u.counters = tw.1g.counters)
Rprof(NULL)
summaryRprof(tmp)

save(tw.1g.terms, tw.1g.counters, tw.1g.V, tw.1g.N, tw.1g.cumulativeCoverage, tw.1g.model,
     file = "./../data/processed/model.twitter.1g.Rdata")


Rprof(tmp <- tempfile())
unigramModel.probabilityForWord(u.model = tw.1g.model, word = "<s>")
Rprof(NULL)
summaryRprof(tmp)


Rprof(tmp <- tempfile())
unigramModel.probabilityForWord(u.model = tw.1g.model, word = "i'm")
Rprof(NULL)
summaryRprof(tmp)

Rprof(tmp <- tempfile())
unigramModel.probabilityForWord(u.model = tw.1g.model, word = "sentimental")
Rprof(NULL)
summaryRprof(tmp)
```

## Bigram Model

```{r bigramModelCreate, collapse = T, eval = F}
## TO BE REFACTORED & REVIEWED
rm(list = ls())
source("./model.R")
load(file = "./../data/processed/model.twitter.1g.Rdata")

tw.2g.list <- load.twitter.2g.data("./../data/processed/")

tw.2g.terms <- rownames(tw.2g.list$allTermsCounters)
tw.2g.counters <- tw.2g.list$allTermsCounters$freq
tw.2g.N <- sum(tw.2g.counters)
tw.2g.V <- length(unique(tw.2g.terms))

#Order by frequency (decreasing)
idx <- order(tw.2g.counters, decreasing = T)
tw.2g.counters <- tw.2g.counters[idx]
tw.2g.terms <- tw.2g.terms[idx]
tw.2g.cumulativeCoverage <- (cumsum(tw.2g.counters)/ tw.2g.N) * 100
idx <- NULL

plot(tw.2g.cumulativeCoverage)

idx <- which(tw.2g.counters > 1)
length(idx) 

idx <- which(tw.2g.counters > 5)
length(idx) 

idx <- which(tw.2g.cumulativeCoverage <= 50)
length(idx) 

##Simplification: consider the bigrams in order to have a max cumulative coverage of%
idx <- which(tw.2g.cumulativeCoverage <= 50)
tw.2g.counters <- tw.2g.counters[idx]
tw.2g.terms <- tw.2g.terms[idx]
tw.2g.cumulativeCoverage <- (cumsum(tw.2g.counters)/ tw.2g.N) * 100
plot(tw.2g.cumulativeCoverage)


##Create the twitter bigram model
Rprof(tmp <- tempfile())
tw.2g.model <- bigrams.model(b.terms = tw.2g.terms, b.counters = tw.2g.counters, 
                             u.words = tw.1g.terms, u.counters = tw.1g.counters)
Rprof(NULL)
summaryRprof(tmp)

save(tw.2g.terms, tw.2g.counters, tw.2g.V, tw.2g.N, tw.2g.cumulativeCoverage, tw.2g.model,
     file = "./../data/processed/model.twitter.2g.Rdata")
```

## Trigram Models

```{r trigramModelTwitter, eval = F}
rm(list = ls())
source("./model.R")

tw.3g <- load.twitter.3g.data(file_url_allTermsFrequency = "./tmp/twitter.sample1.allTermsFrequency.3g.Rdata",
                              file_url_tdm = "./tmp/twitter.sample1.tdm.3g.Rdata")

tw.2g <- load.twitter.2g.data(file_url_allTermsFrequency = "./tmp/twitter.sample1.allTermsFrequency.2g.Rdata",
                              file_url_tdm = "./tmp/twitter.sample1.tdm.2g.Rdata")
##Trigrams info
tw.3g.tc.info <- orderElementsByFrequency(tcv = tw.3g, decreasing = T)
tw.3g.tc.info$terms[1:10]
tw.3g.tc.info$counters[1:10]

##Bigrams info
tw.2g.tc.info <- orderElementsByFrequency(tcv = tw.2g, decreasing = T)
tw.2g.tc.info$terms[1:10]
tw.2g.tc.info$counters[1:10]

# noElements <- 10000
noElements <- 100000
filename <- paste("./tmp/models/twitter.3g.model_", Sys.Date(), "_topTerms.Rdata", sep = "")

tw.3g.model <- trigrams.model(t.terms = tw.3g.tc.info$terms[1:noElements], t.counters = tw.3g.tc.info$counters[1:noElements],
                              b.terms = tw.2g.tc.info$terms, b.counters = tw.2g.tc.info$counters)

tw.3g.model.df <- data.frame(tw.3g.model$t.bigram, tw.3g.model$t.nextWord, tw.3g.model$t.probability)

# save(tw.3g.model, tw.3g.model.df, file = "./tmp/xodels/twitter.3g.model_100000topTerms.Rdata")
save(tw.3g.model, tw.3g.model.df, file = filename)
rm(list = ls())
```

```{r trigramModelNews, eval = F}
rm(list = ls())
source("./model.R")

tw.3g <- load.news.3g.data(file_url_allTermsFrequency = "./tmp/news.sample.allTermsFrequency.3g.Rdata",
                              file_url_tdm = "./tmp/news.sample.tdm.3g.Rdata")

tw.2g <- load.news.2g.data(file_url_allTermsFrequency = "./tmp/news.sample.allTermsFrequency.2g.Rdata",
                              file_url_tdm = "./tmp/news.sample.tdm.2g.Rdata")
##Trigrams info
tw.3g.tc.info <- orderElementsByFrequency(tcv = tw.3g, decreasing = T)
tw.3g.tc.info$terms[1:10]
tw.3g.tc.info$counters[1:10]

##Bigrams info
tw.2g.tc.info <- orderElementsByFrequency(tcv = tw.2g, decreasing = T)
tw.2g.tc.info$terms[1:10]
tw.2g.tc.info$counters[1:10]

# noElements <- 15000
noElements <- 100000
filename <- paste("./tmp/models/news.3g.model_", Sys.Date(), "_topTerms.Rdata", sep = "")

tw.3g.model <- trigrams.model(t.terms = tw.3g.tc.info$terms[1:noElements], t.counters = tw.3g.tc.info$counters[1:noElements],
                              b.terms = tw.2g.tc.info$terms, b.counters = tw.2g.tc.info$counters)

tw.3g.model.df <- data.frame(tw.3g.model$t.bigram, tw.3g.model$t.nextWord, tw.3g.model$t.probability)

n.3g.model <- tw.3g.model
n.3g.model.df <- tw.3g.model.df

save(n.3g.model, n.3g.model.df, file = filename)
rm(list = ls())
```

```{r trigramModelBlogs, eval = F}
rm(list = ls())
source("./model.R")

tw.3g <- load.blogs.3g.data(file_url_allTermsFrequency = "./tmp/blogs.sample.allTermsFrequency.3g.Rdata",
                              file_url_tdm = "./tmp/blogs.sample.tdm.3g.Rdata")

tw.2g <- load.blogs.2g.data(file_url_allTermsFrequency = "./tmp/blogs.sample.allTermsFrequency.2g.Rdata",
                              file_url_tdm = "./tmp/blogs.sample.tdm.2g.Rdata")
##Trigrams info
tw.3g.tc.info <- orderElementsByFrequency(tcv = tw.3g, decreasing = T)
tw.3g.tc.info$terms[1:10]
tw.3g.tc.info$counters[1:10]

##Bigrams info
tw.2g.tc.info <- orderElementsByFrequency(tcv = tw.2g, decreasing = T)
tw.2g.tc.info$terms[1:10]
tw.2g.tc.info$counters[1:10]

tw.3g.tc.info$terms[14980:15000]
tw.3g.tc.info$counters[14980:15000]

tw.3g.tc.info$terms[100000]
tw.3g.tc.info$counters[100000]

# noElements <- 15000
noElements <- 100000
filename <- paste("./tmp/models/blogs.3g.model_", Sys.Date(), "_topTerms.Rdata", sep = "")


tw.3g.model <- trigrams.model(t.terms = tw.3g.tc.info$terms[1:noElements], t.counters = tw.3g.tc.info$counters[1:noElements],
                              b.terms = tw.2g.tc.info$terms, b.counters = tw.2g.tc.info$counters)

tw.3g.model.df <- data.frame(tw.3g.model$t.bigram, tw.3g.model$t.nextWord, tw.3g.model$t.probability)

b.3g.model <- tw.3g.model
b.3g.model.df <- tw.3g.model.df

save(b.3g.model, b.3g.model.df, file = filename)
rm(list = ls())
```

# MOdel Optimized

```{r useModels}
rm(list = ls())
source("model.R")
#(b.3g.model, b.3g.model.df, file = "./tmp/models/blogs.3g.model_10000topTerms.Rdata")
load(file = "./tmp/models/blogs.3g.model_10000topTerms.Rdata")

#(n.3g.model, n.3g.model.df, file = "./tmp/models/news.3g.model_10000topTerms.Rdata")
load(file = "./tmp/models/news.3g.model_10000topTerms.Rdata")

# (tw.3g.model, tw.3g.model.df, file = "./tmp/models/twitter.3g.model_10000topTerms.Rdata")
load(file = "./tmp/models/twitter.3g.model_10000topTerms.Rdata")

bigram <- "<s> thanks"
tw.3g.model.df[which(tw.3g.model.df$tw.3g.model.t.bigram == bigram),]
n.3g.model.df[which(n.3g.model.df$tw.3g.model.t.bigram == bigram),]
b.3g.model.df[which(b.3g.model.df$tw.3g.model.t.bigram == bigram),]

#The guy in front of me just bought a pound of bacon, a bouquet, and a case of
# beer(g)
bigram <- "case of"
tw.3g.model.df[which(tw.3g.model.df$tw.3g.model.t.bigram == bigram),]
n.3g.model.df[which(n.3g.model.df$tw.3g.model.t.bigram == bigram),]
b.3g.model.df[which(b.3g.model.df$tw.3g.model.t.bigram == bigram),]

# You're the reason why I smile everyday. Can you follow me please? It would mean the
# world
bigram <- "mean the"
tw.3g.model.df[which(tw.3g.model.df$tw.3g.model.t.bigram == bigram),]
n.3g.model.df[which(n.3g.model.df$tw.3g.model.t.bigram == bigram),]
b.3g.model.df[which(b.3g.model.df$tw.3g.model.t.bigram == bigram),]

# Hey sunshine, can you follow me and make me the
# Happiest(g)
bigram <- "me the"
tw.3g.model.df[which(tw.3g.model.df$tw.3g.model.t.bigram == bigram),]
n.3g.model.df[which(n.3g.model.df$tw.3g.model.t.bigram == bigram),]
b.3g.model.df[which(b.3g.model.df$tw.3g.model.t.bigram == bigram),]

# Very early observations on the Bills game: Offense still struggling but the
# defense(g)
bigram <- "but the"
tw.3g.model.df[which(tw.3g.model.df$tw.3g.model.t.bigram == bigram),]
n.3g.model.df[which(n.3g.model.df$tw.3g.model.t.bigram == bigram),]
b.3g.model.df[which(b.3g.model.df$tw.3g.model.t.bigram == bigram),]

# Go on a romantic date at the
# beach(g)
bigram <- "at the"
tw.3g.model.df[which(tw.3g.model.df$tw.3g.model.t.bigram == bigram),]
n.3g.model.df[which(n.3g.model.df$tw.3g.model.t.bigram == bigram),]
b.3g.model.df[which(b.3g.model.df$tw.3g.model.t.bigram == bigram),]

# Well I'm pretty sure my granny has some old bagpipes in her garage I'll dust them off and be on my
# way(g)
bigram <- "on my"
tw.3g.model.df[which(tw.3g.model.df$tw.3g.model.t.bigram == bigram),]
n.3g.model.df[which(n.3g.model.df$tw.3g.model.t.bigram == bigram),]
b.3g.model.df[which(b.3g.model.df$tw.3g.model.t.bigram == bigram),]

# Ohhhhh #PointBreak is on tomorrow. Love that film and haven't seen it in quite some
# time
bigram <- "quite some"
tw.3g.model.df[which(tw.3g.model.df$tw.3g.model.t.bigram == bigram),]
n.3g.model.df[which(n.3g.model.df$tw.3g.model.t.bigram == bigram),]
b.3g.model.df[which(b.3g.model.df$tw.3g.model.t.bigram == bigram),]


# After the ice bucket challenge Louis will push his long wet hair out of his eyes with his little
# fingers(g)
bigram <- "his little"
tw.3g.model.df[which(tw.3g.model.df$tw.3g.model.t.bigram == bigram),]
n.3g.model.df[which(n.3g.model.df$tw.3g.model.t.bigram == bigram),]
b.3g.model.df[which(b.3g.model.df$tw.3g.model.t.bigram == bigram),]


# Be grateful for the good times and keep the faith during the
# bad(g)
bigram <- "during the"
tw.3g.model.df[which(tw.3g.model.df$tw.3g.model.t.bigram == bigram),]
n.3g.model.df[which(n.3g.model.df$tw.3g.model.t.bigram == bigram),]
b.3g.model.df[which(b.3g.model.df$tw.3g.model.t.bigram == bigram),]


# If this isn't the cutest thing you've ever seen, then you must be
# insane(g)
bigram <- "must be"
tw.3g.model.df[which(tw.3g.model.df$tw.3g.model.t.bigram == bigram),]
n.3g.model.df[which(n.3g.model.df$tw.3g.model.t.bigram == bigram),]
b.3g.model.df[which(b.3g.model.df$tw.3g.model.t.bigram == bigram),]



sentenceProbability <- function(model.df, trigrams){
    size <- length(trigrams)
    result <- 0
    for(i in 1:size){
        el.words <- ngramTokenize(trigrams[i],1)
        el.b <- paste(el.words[1], el.words[2])
        el.nw <- el.words[3]
        el.p <- model.df$tw.3g.model.t.probability[
            which(model.df$tw.3g.model.t.bigram == el.b & model.df$tw.3g.model.t.nextWord == el.nw)]
        print(paste(el.b,"::", el.nw, "::", el.p, sep = ""))
        result <- result * el.p
    }
    result
}

#haven't seen it in quite some time
x <- c("haven't seen it", "seen it in", "it in quite", "in quite some", "quite some time")
sentenceProbability(model.df = tw.3g.model.df, trigrams = x)
sentenceProbability(model.df = n.3g.model.df, trigrams = x)
sentenceProbability(model.df = b.3g.model.df, trigrams = x)

bigram <- "haven't seen"
tw.3g.model.df[which(tw.3g.model.df$tw.3g.model.t.bigram == bigram),]
n.3g.model.df[which(n.3g.model.df$tw.3g.model.t.bigram == bigram),]
b.3g.model.df[which(b.3g.model.df$tw.3g.model.t.bigram == bigram),]

bigram <- "quite some"
tw.3g.model.df[which(tw.3g.model.df$tw.3g.model.t.bigram == bigram),]
n.3g.model.df[which(n.3g.model.df$tw.3g.model.t.bigram == bigram),]
b.3g.model.df[which(b.3g.model.df$tw.3g.model.t.bigram == bigram),]


```

# Collapsing the ngrams data into a signle list for uni, bi and tri 
A single set of list for all of the Corpora

Work In progress

```{r collapse, eval=F}
rm(list = ls())
source("model.R")
tw.3g <- load.twitter.3g.data(file_url_allTermsFrequency = "./tmp/twitter.sample1.allTermsFrequency.3g.Rdata",
                              file_url_tdm = "./tmp/twitter.sample1.tdm.3g.Rdata")

n.3g <- load.news.3g.data(file_url_allTermsFrequency = "./tmp/news.sample.allTermsFrequency.3g.Rdata",
                              file_url_tdm = "./tmp/news.sample.tdm.3g.Rdata")

b.3g <- load.blogs.3g.data(file_url_allTermsFrequency = "./tmp/blogs.sample.allTermsFrequency.3g.Rdata",
                              file_url_tdm = "./tmp/blogs.sample.tdm.3g.Rdata")

##Ordering
tw.3g.tc.info <- orderElementsByFrequency(tcv = tw.3g, decreasing = T)
n.3g.tc.info <- orderElementsByFrequency(tcv = n.3g, decreasing = T)
b.3g.tc.info <- orderElementsByFrequency(tcv = b.3g, decreasing = T)


d1.df <- data.frame(terms = tw.3g.tc.info$terms, counters = tw.3g.tc.info$counters, stringsAsFactors = F)
d2.df <- data.frame(terms = n.3g.tc.info$terms, counters = n.3g.tc.info$counters,stringsAsFactors = F)
d3.df <- data.frame(terms = b.3g.tc.info$terms, counters = b.3g.tc.info$counters, stringsAsFactors = F)
d.3g.df <- merge(d1.df, d2.df, by = "terms", all = T)
d.3g.df <- merge(d.3g.df, d3.df, by = "terms", all = T)

d.3g.df$counters.x[is.na(d.3g.df$counters.x)] = 0
d.3g.df$counters.y[is.na(d.3g.df$counters.y)] = 0
d.3g.df$counters[is.na(d.3g.df$counters)] = 0
d.3g.df$total = d.3g.df$counters.x + d.3g.df$counters.y + d.3g.df$counters

#x <- d.3g.df$terms[grep("'{2,}", d.3g.df$terms)]

save(d.3g.df, file = "./tmp/aggregated_allTermsFrequency.3g.Rdata")


rm(list = ls())
source("model.R")

tw.2g <- load.twitter.2g.data(file_url_allTermsFrequency = "./tmp/twitter.sample1.allTermsFrequency.2g.Rdata",
                              file_url_tdm = "./tmp/twitter.sample1.tdm.2g.Rdata")

n.2g <- load.news.2g.data(file_url_allTermsFrequency = "./tmp/news.sample.allTermsFrequency.2g.Rdata",
                              file_url_tdm = "./tmp/news.sample.tdm.2g.Rdata")

b.2g <- load.blogs.2g.data(file_url_allTermsFrequency = "./tmp/blogs.sample.allTermsFrequency.2g.Rdata",
                              file_url_tdm = "./tmp/blogs.sample.tdm.2g.Rdata")

##Ordering
tw.2g.tc.info <- orderElementsByFrequency(tcv = tw.2g, decreasing = T)
n.2g.tc.info <- orderElementsByFrequency(tcv = n.2g, decreasing = T)
b.2g.tc.info <- orderElementsByFrequency(tcv = b.2g, decreasing = T)


d1.df <- data.frame(terms = tw.2g.tc.info$terms, counters = tw.2g.tc.info$counters, stringsAsFactors = F)
d2.df <- data.frame(terms = n.2g.tc.info$terms, counters = n.2g.tc.info$counters,stringsAsFactors = F)
d3.df <- data.frame(terms = b.2g.tc.info$terms, counters = b.2g.tc.info$counters, stringsAsFactors = F)
d.2g.df <- merge(d1.df, d2.df, by = "terms", all = T)
d.2g.df <- merge(d.2g.df, d3.df, by = "terms", all = T)

d.2g.df$counters.x[is.na(d.2g.df$counters.x)] = 0
d.2g.df$counters.y[is.na(d.2g.df$counters.y)] = 0
d.2g.df$counters[is.na(d.2g.df$counters)] = 0
d.2g.df$total = d.2g.df$counters.x + d.2g.df$counters.y + d.2g.df$counters

#x <- d.2g.df$terms[grep("'{2,}", d.2g.df$terms)]

save(d.2g.df, file = "./tmp/aggregated_allTermsFrequency.2g.Rdata")

rm(list = ls())
source("model.R")

tw.1g <- load.twitter.1g.data(file_url_allTermsFrequency = "./tmp/twitter.sample1.allTermsFrequency.1g.Rdata",
                              file_url_tdm = "./tmp/twitter.sample1.tdm.1g.Rdata")

n.1g <- load.news.1g.data(file_url_allTermsFrequency = "./tmp/news.sample.allTermsFrequency.1g.Rdata",
                              file_url_tdm = "./tmp/news.sample.tdm.1g.Rdata")

b.1g <- load.blogs.1g.data(file_url_allTermsFrequency = "./tmp/blogs.sample.allTermsFrequency.1g.Rdata",
                              file_url_tdm = "./tmp/blogs.sample.tdm.1g.Rdata")

##Ordering
tw.1g.tc.info <- orderElementsByFrequency(tcv = tw.1g, decreasing = T)
n.1g.tc.info <- orderElementsByFrequency(tcv = n.1g, decreasing = T)
b.1g.tc.info <- orderElementsByFrequency(tcv = b.1g, decreasing = T)


d1.df <- data.frame(terms = tw.1g.tc.info$terms, counters = tw.1g.tc.info$counters, stringsAsFactors = F)
d2.df <- data.frame(terms = n.1g.tc.info$terms, counters = n.1g.tc.info$counters,stringsAsFactors = F)
d3.df <- data.frame(terms = b.1g.tc.info$terms, counters = b.1g.tc.info$counters, stringsAsFactors = F)
d.1g.df <- merge(d1.df, d2.df, by = "terms", all = T)
d.1g.df <- merge(d.1g.df, d3.df, by = "terms", all = T)

d.1g.df$counters.x[is.na(d.1g.df$counters.x)] = 0
d.1g.df$counters.y[is.na(d.1g.df$counters.y)] = 0
d.1g.df$counters[is.na(d.1g.df$counters)] = 0
d.1g.df$total = d.1g.df$counters.x + d.1g.df$counters.y + d.1g.df$counters

#x <- d.1g.df$terms[grep("'{2,}", d.1g.df$terms)]

save(d.1g.df, file = "./tmp/aggregated_allTermsFrequency.1g.Rdata")
```



# Using the terms/ counters vector directly

## StupidBAckoff model

Challenges 
__merging of the different terms/counters vector for twitter/ news/ blogs__for

unigrams
bigrams
trigrams

## SImple experiment using only twitter data using the stupid backoff algorithm

```{r}
rm(list = ls())
source("model.R")
require(hash)
tw.3g <- load.twitter.3g.data(file_url_allTermsFrequency = "./tmp/twitter.sample1.allTermsFrequency.3g.Rdata",
                              file_url_tdm = "./tmp/twitter.sample1.tdm.3g.Rdata")

tw.2g <- load.twitter.2g.data(file_url_allTermsFrequency = "./tmp/twitter.sample1.allTermsFrequency.2g.Rdata",
                              file_url_tdm = "./tmp/twitter.sample1.tdm.2g.Rdata")

tw.1g <- load.twitter.1g.data(file_url_allTermsFrequency = "./tmp/twitter.sample1.allTermsFrequency.1g.Rdata",
                              file_url_tdm = "./tmp/twitter.sample1.tdm.1g.Rdata")
##Ordering
tw.3g.tc.info <- orderElementsByFrequency(tcv = tw.3g, decreasing = T)
tw.2g.tc.info <- orderElementsByFrequency(tcv = tw.2g, decreasing = T)
tw.1g.tc.info <- orderElementsByFrequency(tcv = tw.1g, decreasing = T)

format(object.size(tw.3g.tc.info$terms), "auto")
format(object.size(tw.3g.tc.info$counters), "auto")

#Something to investigate teh use of hash for memory reduction
tw.3g.h <- hash(tw.3g.tc.info$terms, tw.3g.tc.info$counters)
format(object.size(tw.3g.h), "auto")

tw.3g.h[tw.3g.tc.info$terms[1:10]]
tw.3g.tc.info$terms[1:10]
tw.3g.tc.info$counters[1:10]


format(object.size(tw.2g.tc.info$terms), "auto")
format(object.size(tw.2g.tc.info$counters), "auto")
format(object.size(tw.1g.tc.info$terms), "auto")
format(object.size(tw.1g.tc.info$counters), "auto")


tw.2g.tc.info$terms[1:10]
tw.2g.tc.info$counters[1:10]

tw.1g.tc.info$terms[1:10]
tw.1g.tc.info$counters[1:10]

load(file = "./tmp/twitter_step2_sample.1.Rdata")
# data.twitter.sample[10:20]


s <- "<s> you will absolutely love it after the"

Rprof(tmp <- tempfile())
sentenceProbability(s = s, t.terms = tw.3g.tc.info$terms, t.counters = tw.3g.tc.info$counters,
                    b.terms = tw.2g.tc.info$terms, tw.2g.tc.info$counters,
                    u.words = tw.1g.tc.info$terms, u.counters = tw.1g.tc.info$counters)
Rprof(NULL)
summaryRprof(tmp)

s <- "if this isn't the cutest thing you've ever seen then you must be insane"
Rprof(tmp <- tempfile())
sentenceProbability(s = s, t.terms = tw.3g.tc.info$terms, t.counters = tw.3g.tc.info$counters,
                    b.terms = tw.2g.tc.info$terms, tw.2g.tc.info$counters,
                    u.words = tw.1g.tc.info$terms, u.counters = tw.1g.tc.info$counters)
Rprof(NULL)
summaryRprof(tmp)

```

