---
title: "Language Models"
author: "Pier Lorenzo Paracchini"
date: "3 mai 2016"
output: 
  html_document: 
    keep_md: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# WINDOWS LOCALE SETTING
Sys.setlocale(category = "LC_ALL",locale = "English_United States.1252")

require(profr)
require(hash)
```

# Language Model Creation - some Notes & The Process

# 1st Approach Trigram Models

See `./trigramModel` folder & `model_trigramLanguageModel.R` file


Initial approach -> create a simplify data structure for the trigram model
bigram, nextWord, counter, probability

The processing time is demanding and for this reason the size of element processed has been ad-hoc selected to a certain number of element (most frequent first). Still we have three models one for each corpora - Twitter, News and Blog. The three model shoudl be collapsed into only one table.

Process

- generate the model (twitter, news, blogs) / need to focus ON PERFORMANCE OF THE CODE _(NOT VERY EFFICIENT)_
- merge the models into 1 big model (TODO)
- evaluate the model (probability/ Perplexity on a common test data set)

__Note the generation of the models depends on the normalized coprpora so when re-cleaning & processing the corpora all of those steps need to be performed.__


```{r trigramModels, eval = F}
## Twitter
rm(list = ls())
source("./model_trigramLanguageModel.R")

noElements = 100000
filename.3g <- generate.twitter.3gramModel(folder = "./../data/processed/",
                            file.allTermFrequency.3g = "twitter.sample1.allTermsFrequency.3g.Rdata",
                            file.tdm.3g = "twitter.sample1.tdm.3g.Rdata",
                            file.allTermFrequency.2g = "twitter.sample1.allTermsFrequency.2g.Rdata",
                            file.tdm.2g = "twitter.sample1.tdm.2g.Rdata",
                            noElements = noElements)


# load(filename.3g)
# data.3g.tc.info$terms[1:10]
# data.3g.tc.info$counters[1:10]
# data.2g.tc.info$terms[1:10]
# data.2g.tc.info$counters[1:10]
# data.3g.model.df[1:10,]

## NEWS Corpora
rm(list = ls())
source("./model_trigramLanguageModel.R")

noElements = 100000
filename.3g <- generate.news.3gramModel(folder = "./../data/processed/",
                            file.allTermFrequency.3g = "news.sample.allTermsFrequency.3g.Rdata",
                            file.tdm.3g = "news.sample.tdm.3g.Rdata",
                            file.allTermFrequency.2g = "news.sample.allTermsFrequency.2g.Rdata",
                            file.tdm.2g = "news.sample.tdm.2g.Rdata",
                            noElements = noElements)

#load(filename.3g)
# data.3g.tc.info$terms[1:10]
# data.3g.tc.info$counters[1:10]
# data.2g.tc.info$terms[1:10]
# data.2g.tc.info$counters[1:10]
# data.3g.model.df[1:10,]


## BLOGS Corpora
rm(list = ls())
source("./model_trigramLanguageModel.R")

noElements = 100000
filename.3g <- generate.blogs.3gramModel(folder = "./../data/processed/",
                            file.allTermFrequency.3g = "blogs.sample.allTermsFrequency.3g.Rdata",
                            file.tdm.3g = "blogs.sample.tdm.3g.Rdata",
                            file.allTermFrequency.2g = "blogs.sample.allTermsFrequency.2g.Rdata",
                            file.tdm.2g = "blogs.sample.tdm.2g.Rdata",
                            noElements = noElements)

# load(filename.3g)
# data.3g.tc.info$terms[1:10]
# data.3g.tc.info$counters[1:10]
# data.2g.tc.info$terms[1:10]
# data.2g.tc.info$counters[1:10]
# data.3g.model.df[1:10,]

rm(list = ls())
```


```{r useModels, eval = F}
rm(list = ls())
source("model.R")
#(b.3g.model, b.3g.model.df, file = "./tmp/models/blogs.3g.model_10000topTerms.Rdata")
load(file = "./tmp/models/blogs.3g.model_10000topTerms.Rdata")

#(n.3g.model, n.3g.model.df, file = "./tmp/models/news.3g.model_10000topTerms.Rdata")
load(file = "./tmp/models/news.3g.model_10000topTerms.Rdata")

# (tw.3g.model, tw.3g.model.df, file = "./tmp/models/twitter.3g.model_10000topTerms.Rdata")
load(file = "./tmp/models/twitter.3g.model_10000topTerms.Rdata")

bigram <- "<s> thanks"
tw.3g.model.df[which(tw.3g.model.df$tw.3g.model.t.bigram == bigram),]
n.3g.model.df[which(n.3g.model.df$tw.3g.model.t.bigram == bigram),]
b.3g.model.df[which(b.3g.model.df$tw.3g.model.t.bigram == bigram),]

#The guy in front of me just bought a pound of bacon, a bouquet, and a case of
# beer(g)
bigram <- "case of"
tw.3g.model.df[which(tw.3g.model.df$tw.3g.model.t.bigram == bigram),]
n.3g.model.df[which(n.3g.model.df$tw.3g.model.t.bigram == bigram),]
b.3g.model.df[which(b.3g.model.df$tw.3g.model.t.bigram == bigram),]

# You're the reason why I smile everyday. Can you follow me please? It would mean the
# world
bigram <- "mean the"
tw.3g.model.df[which(tw.3g.model.df$tw.3g.model.t.bigram == bigram),]
n.3g.model.df[which(n.3g.model.df$tw.3g.model.t.bigram == bigram),]
b.3g.model.df[which(b.3g.model.df$tw.3g.model.t.bigram == bigram),]

# Hey sunshine, can you follow me and make me the
# Happiest(g)
bigram <- "me the"
tw.3g.model.df[which(tw.3g.model.df$tw.3g.model.t.bigram == bigram),]
n.3g.model.df[which(n.3g.model.df$tw.3g.model.t.bigram == bigram),]
b.3g.model.df[which(b.3g.model.df$tw.3g.model.t.bigram == bigram),]

# Very early observations on the Bills game: Offense still struggling but the
# defense(g)
bigram <- "but the"
tw.3g.model.df[which(tw.3g.model.df$tw.3g.model.t.bigram == bigram),]
n.3g.model.df[which(n.3g.model.df$tw.3g.model.t.bigram == bigram),]
b.3g.model.df[which(b.3g.model.df$tw.3g.model.t.bigram == bigram),]

# Go on a romantic date at the
# beach(g)
bigram <- "at the"
tw.3g.model.df[which(tw.3g.model.df$tw.3g.model.t.bigram == bigram),]
n.3g.model.df[which(n.3g.model.df$tw.3g.model.t.bigram == bigram),]
b.3g.model.df[which(b.3g.model.df$tw.3g.model.t.bigram == bigram),]

# Well I'm pretty sure my granny has some old bagpipes in her garage I'll dust them off and be on my
# way(g)
bigram <- "on my"
tw.3g.model.df[which(tw.3g.model.df$tw.3g.model.t.bigram == bigram),]
n.3g.model.df[which(n.3g.model.df$tw.3g.model.t.bigram == bigram),]
b.3g.model.df[which(b.3g.model.df$tw.3g.model.t.bigram == bigram),]

# Ohhhhh #PointBreak is on tomorrow. Love that film and haven't seen it in quite some
# time
bigram <- "quite some"
tw.3g.model.df[which(tw.3g.model.df$tw.3g.model.t.bigram == bigram),]
n.3g.model.df[which(n.3g.model.df$tw.3g.model.t.bigram == bigram),]
b.3g.model.df[which(b.3g.model.df$tw.3g.model.t.bigram == bigram),]


# After the ice bucket challenge Louis will push his long wet hair out of his eyes with his little
# fingers(g)
bigram <- "his little"
tw.3g.model.df[which(tw.3g.model.df$tw.3g.model.t.bigram == bigram),]
n.3g.model.df[which(n.3g.model.df$tw.3g.model.t.bigram == bigram),]
b.3g.model.df[which(b.3g.model.df$tw.3g.model.t.bigram == bigram),]


# Be grateful for the good times and keep the faith during the
# bad(g)
bigram <- "during the"
tw.3g.model.df[which(tw.3g.model.df$tw.3g.model.t.bigram == bigram),]
n.3g.model.df[which(n.3g.model.df$tw.3g.model.t.bigram == bigram),]
b.3g.model.df[which(b.3g.model.df$tw.3g.model.t.bigram == bigram),]


# If this isn't the cutest thing you've ever seen, then you must be
# insane(g)
bigram <- "must be"
tw.3g.model.df[which(tw.3g.model.df$tw.3g.model.t.bigram == bigram),]
n.3g.model.df[which(n.3g.model.df$tw.3g.model.t.bigram == bigram),]
b.3g.model.df[which(b.3g.model.df$tw.3g.model.t.bigram == bigram),]



sentenceProbability <- function(model.df, trigrams){
    size <- length(trigrams)
    result <- 0
    for(i in 1:size){
        el.words <- ngramTokenize(trigrams[i],1)
        el.b <- paste(el.words[1], el.words[2])
        el.nw <- el.words[3]
        el.p <- model.df$tw.3g.model.t.probability[
            which(model.df$tw.3g.model.t.bigram == el.b & model.df$tw.3g.model.t.nextWord == el.nw)]
        print(paste(el.b,"::", el.nw, "::", el.p, sep = ""))
        result <- result * el.p
    }
    result
}

#haven't seen it in quite some time
x <- c("haven't seen it", "seen it in", "it in quite", "in quite some", "quite some time")
sentenceProbability(model.df = tw.3g.model.df, trigrams = x)
sentenceProbability(model.df = n.3g.model.df, trigrams = x)
sentenceProbability(model.df = b.3g.model.df, trigrams = x)

bigram <- "haven't seen"
tw.3g.model.df[which(tw.3g.model.df$tw.3g.model.t.bigram == bigram),]
n.3g.model.df[which(n.3g.model.df$tw.3g.model.t.bigram == bigram),]
b.3g.model.df[which(b.3g.model.df$tw.3g.model.t.bigram == bigram),]

bigram <- "quite some"
tw.3g.model.df[which(tw.3g.model.df$tw.3g.model.t.bigram == bigram),]
n.3g.model.df[which(n.3g.model.df$tw.3g.model.t.bigram == bigram),]
b.3g.model.df[which(b.3g.model.df$tw.3g.model.t.bigram == bigram),]


```

# Collapsing the ngrams data into a signle list for uni, bi and tri 
A single set of list for all of the Corpora for 
unigrams,
Bigrams and
Trigrams



```{r collapse, eval=F}
rm(list = ls())
source("model.R")
tw.3g <- load.twitter.3g.data(file_url_allTermsFrequency = "./../data/processed/twitter.sample1.allTermsFrequency.3g.Rdata",
                              file_url_tdm = "./../data/processed/twitter.sample1.tdm.3g.Rdata")

n.3g <- load.news.3g.data(file_url_allTermsFrequency = "./../data/processed/news.sample.allTermsFrequency.3g.Rdata",
                              file_url_tdm = "./../data/processed/news.sample.tdm.3g.Rdata")

b.3g <- load.blogs.3g.data(file_url_allTermsFrequency = "./../data/processed/blogs.sample.allTermsFrequency.3g.Rdata",
                              file_url_tdm = "./../data/processed/blogs.sample.tdm.3g.Rdata")

filename <- collapseToOneList(twitter.ng = tw.3g, news.dg = n.3g, blogs.ng = b.3g, ng = 3)

# load(filename)
# x <- d.ng.df$terms[grep("'{2,}", d.ng.df$terms)]
# head(x)
# tail(x)

rm(list = ls())
source("model.R")

tw.2g <- load.twitter.2g.data(file_url_allTermsFrequency = "./../data/processed/twitter.sample1.allTermsFrequency.2g.Rdata",
                              file_url_tdm = "./../data/processed/twitter.sample1.tdm.2g.Rdata")

n.2g <- load.news.2g.data(file_url_allTermsFrequency = "./../data/processed/news.sample.allTermsFrequency.2g.Rdata",
                              file_url_tdm = "./../data/processed/news.sample.tdm.2g.Rdata")

b.2g <- load.blogs.2g.data(file_url_allTermsFrequency = "./../data/processed/blogs.sample.allTermsFrequency.2g.Rdata",
                              file_url_tdm = "./../data/processed/blogs.sample.tdm.2g.Rdata")

filename <- collapseToOneList(twitter.ng = tw.2g, news.dg = n.2g, blogs.ng = b.2g, ng = 2)

# load(filename)
# x <- d.ng.df$terms[grep("'{2,}", d.ng.df$terms)]
# head(x)
# tail(x)

rm(list = ls())
source("model.R")

tw.1g <- load.twitter.1g.data(file_url_allTermsFrequency = "./../data/processed/twitter.sample1.allTermsFrequency.1g.Rdata",
                              file_url_tdm = "./../data/processed/twitter.sample1.tdm.1g.Rdata")

n.1g <- load.news.1g.data(file_url_allTermsFrequency = "./../data/processed/news.sample.allTermsFrequency.1g.Rdata",
                              file_url_tdm = "./../data/processed/news.sample.tdm.1g.Rdata")

b.1g <- load.blogs.1g.data(file_url_allTermsFrequency = "./../data/processed/blogs.sample.allTermsFrequency.1g.Rdata",
                              file_url_tdm = "./../data/processed/blogs.sample.tdm.1g.Rdata")

filename <- collapseToOneList(twitter.ng = tw.1g, news.dg = n.1g, blogs.ng = b.1g, ng = 1)

# load(filename)
# x <- d.ng.df$terms[grep("'{2,}", d.ng.df$terms)]
# head(x)
# tail(x)
```



# Another Approach for the model - Use the terms/ counters vector directly

## StupidBackoff model

## SImple experiment using only twitter data using the stupid backoff algorithm

```{r}
rm(list = ls())
source("model.R")
require(hash)
# tw.3g <- load.twitter.3g.data(file_url_allTermsFrequency = "./tmp/twitter.sample1.allTermsFrequency.3g.Rdata",
#                               file_url_tdm = "./tmp/twitter.sample1.tdm.3g.Rdata")
# 
# tw.2g <- load.twitter.2g.data(file_url_allTermsFrequency = "./tmp/twitter.sample1.allTermsFrequency.2g.Rdata",
#                               file_url_tdm = "./tmp/twitter.sample1.tdm.2g.Rdata")
# 
# tw.1g <- load.twitter.1g.data(file_url_allTermsFrequency = "./tmp/twitter.sample1.allTermsFrequency.1g.Rdata",
#                               file_url_tdm = "./tmp/twitter.sample1.tdm.1g.Rdata")
# ##Ordering
# tw.3g.tc.info <- orderElementsByFrequency(tcv = tw.3g, decreasing = T)
# tw.2g.tc.info <- orderElementsByFrequency(tcv = tw.2g, decreasing = T)
# tw.1g.tc.info <- orderElementsByFrequency(tcv = tw.1g, decreasing = T)

load("./../data/processed/allCorpora_aggregated_allTermsFrequency.3g.rdata")
allCorpora.3g <- d.ng.df
d.ng.df <- NULL

load("./../data/processed/allCorpora_aggregated_allTermsFrequency.2g.rdata")
allCorpora.2g <- d.ng.df
d.ng.df <- NULL

load("./../data/processed/allCorpora_aggregated_allTermsFrequency.1g.rdata")
allCorpora.1g <- d.ng.df
d.ng.df <- NULL

format(object.size(allCorpora.3g$terms), "auto")
format(object.size(allCorpora.3g$total), "auto")

format(object.size(allCorpora.2g$terms), "auto")
format(object.size(allCorpora.2g$total), "auto")

format(object.size(allCorpora.1g$terms), "auto")
format(object.size(allCorpora.1g$total), "auto")

#Something to investigate the use of hash for memory reduction
allCorpora.3g.h <- hash(allCorpora.3g$terms, allCorpora.3g$total)
format(object.size(allCorpora.3g.h), "auto")

allCorpora.3g.h[allCorpora.3g$terms[1:10]]
allCorpora.3g$terms[1:10]
allCorpora.3g$total[1:10]

allCorpora.2g.h <- hash(allCorpora.2g$terms, allCorpora.2g$total)
format(object.size(allCorpora.2g.h), "auto")

allCorpora.2g.h[allCorpora.2g$terms[1:10]]
allCorpora.2g$terms[1:10]
allCorpora.2g$total[1:10]

allCorpora.1g.h <- hash(allCorpora.1g$terms, allCorpora.1g$total)
format(object.size(allCorpora.1g.h), "auto")

allCorpora.1g.h[allCorpora.1g$terms[1:10]]
allCorpora.1g$terms[1:10]
allCorpora.1g$total[1:10]

load(file = "./tmp/twitter_step2_sample.1.Rdata")
data.twitter.sample[10:20]


s <- "<s> you will absolutely love it after the"

Rprof(tmp <- tempfile())
sentenceProbability(s = s, t.terms = allCorpora.3g$terms, t.counters = allCorpora.3g$total,
                    b.terms = allCorpora.2g$terms, allCorpora.2g$total,
                    u.words = allCorpora.1g$terms, u.counters = allCorpora.1g$total)
Rprof(NULL)
summaryRprof(tmp)

s <- "if this isn't the cutest thing you've ever seen then you must be insane"
Rprof(tmp <- tempfile())
sentenceProbability(s = s, t.terms = allCorpora.3g$terms, t.counters = allCorpora.3g$total,
                    b.terms = allCorpora.2g$terms, allCorpora.2g$total,
                    u.words = allCorpora.1g$terms, u.counters = allCorpora.1g$total)
Rprof(NULL)
summaryRprof(tmp)


```

