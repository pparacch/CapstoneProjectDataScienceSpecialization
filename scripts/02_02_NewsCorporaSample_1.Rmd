---
title: "News Corpora Creation"
author: "Pier Lorenzo Paracchini"
date: "4 mai 2016"
output: 
  html_document: 
    keep_md: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)


#Required Libraries
require(knitr)
require(tm)
require(wordcloud)
require(RWeka)
require(ggplot2)
require(RColorBrewer)

# WINDOWS LOCALE SETTING
Sys.setlocale(category = "LC_ALL",locale = "English_United States.1252")
Sys.getlocale("LC_CTYPE")
localeToCharset()
```

```{r loadTheData, eval = F}
rm(list = ls())
source("./corpora.R")

con <- file("./../data/original/final/en_US/en_US.news.txt", "r") 
data.news.all <- readLines(con, skipNul = T)
close(con)

data.news.all.nchar <- nchar(data.news.all)

data.news.all[200:203]

##Using iconv to replace non ASCII char with an empty char
data.news.all.ascii <-  iconv(data.news.all, from = localeToCharset(), to = "ASCII", "")


##Reset working data after removing Gremlings
data.news.all <- data.news.all.ascii
data.news.all.nchar <- nchar(data.news.all)

data.news.all[200:203]


#Let's focus on news
a <- data.news.all.nchar < 1500
par(mfrow=c(1,2))
hist(nchar(data.news.all[a]), main = "No Of Chars per News (< 1500)", breaks = 100, xlab = "no of characters")
hist(nchar(data.news.all[!a]), main = "No Of Chars per News (>= 1500)", breaks = 100, xlab = "no of characters")

shortNews.maxNoOfChar <- 20
a <- data.news.all.nchar <= shortNews.maxNoOfChar
head(data.news.all[a], 10)

#Removing such short news
data.news.all <- data.news.all[data.news.all.nchar > shortNews.maxNoOfChar]
data.news.all.nchar <- nchar(data.news.all)

head(data.news.all, 10)

## REmove Links
data.news.all <- remove_links(texts =  data.news.all)
tail(data.news.all)

## Remove Contractions
data.news.all <- remove_contractions(theTexts = data.news.all)
tail(data.news.all)

data.news.all.df <- data.frame(data.news.all)
save(data.news.all, data.news.all.df, file = "./tmp/news_step1_all.Rdata")
rm(list = ls())
```

```{r dataReduction, eval = F}
rm(list = ls())
source("corpora.R")
load(file = "./tmp/news_step1_all.Rdata")

set.seed(19711004)
coin.biased.outcome <- rbinom(length(data.news.all), 1, 0.05)
#table(coin.biased.outcome)
data.news.sample <- data.news.all[coin.biased.outcome == 1]

save(data.news.sample, file = "./tmp/news_step2_sample.Rdata")
rm(list = ls())
```

```{r generateTermDocumentMatrixAndSummaryHeavyProcessing1, cache = T, echo = F, eval = F}
rm(list = ls())
source("corpora.R")
load(file = "./tmp/news_step2_sample.Rdata")

#Unigrams
news.corpora.transformed <- corpora.transform(data.news.sample)
lapply(news.corpora.transformed[1:5], as.character)
lapply(news.corpora.transformed[100:110], as.character)
save(news.corpora.transformed, file = "./tmp/news.sample.corpora.transformed.Rdata")

news.corpora.tdm.1g <- tdm.generate.ng(news.corpora.transformed)
news.corpora.tdm.1g
lapply(news.corpora.transformed[1:5], as.character)
save(news.corpora.tdm.1g, file = "./tmp/news.sample.tdm.1g.Rdata")
rm(news.corpora.tdm.1g)


## Bi-grams
news.corpora.tdm.2g <- tdm.generate.ng(news.corpora.transformed,ng = 2)
news.corpora.tdm.2g
lapply(news.corpora.transformed[1:5], as.character)
save(news.corpora.tdm.2g, file = "./tmp/news.sample.tdm.2g.Rdata")
rm(news.corpora.tdm.2g)

## Tri-grams
news.corpora.tdm.3g <- tdm.generate.ng(news.corpora.transformed,ng = 3)
news.corpora.tdm.3g
lapply(news.corpora.transformed[1:5], as.character)
save(news.corpora.tdm.3g, file = "./tmp/news.sample.tdm.3g.Rdata")
rm(news.corpora.tdm.3g)

rm(list = ls())
```

```{r calculateTermFrequency, echo = F, eval = F}
rm(list = ls())
source("corpora.R")

##News Corpora
load("./tmp/news.sample.tdm.1g.Rdata")
corpora.allTermsFrequency <- NULL
news.corpora.tdm.1g
corpora.tdm <- news.corpora.tdm.1g
corpora.allTermsFrequency <- getAllTermsFrequencyInCorpora.as.df(corpora.tdm = corpora.tdm, chunck = 2000)
save(corpora.allTermsFrequency, file = "./tmp/news.sample.allTermsFrequency.1g.Rdata")
rm(news.corpora.tdm.1g)

load("./tmp/news.sample.tdm.2g.Rdata")
corpora.allTermsFrequency <- NULL
news.corpora.tdm.2g
corpora.tdm <- news.corpora.tdm.2g
corpora.allTermsFrequency <- getAllTermsFrequencyInCorpora.as.df(corpora.tdm = corpora.tdm, chunck = 2000)
save(corpora.allTermsFrequency, file = "./tmp/news.sample.allTermsFrequency.2g.Rdata")
rm(news.corpora.tdm.2g)

load("./tmp/news.sample.tdm.3g.Rdata")
corpora.allTermsFrequency <- NULL
corpora.tdm <- news.corpora.tdm.3g
corpora.allTermsFrequency <- getAllTermsFrequencyInCorpora.as.df(corpora.tdm = corpora.tdm, chunck = 2000)
save(corpora.allTermsFrequency, file = "./tmp/news.sample.allTermsFrequency.3g.Rdata")
rm(news.corpora.tdm.3g)

corpora.allTermsFrequency <- NULL
corpora.tdm <- NULL

```

## News Corpora

```{r loadNewsData}
rm(list = ls())
source("corpora.R")
load("./tmp/news.sample.tdm.1g.Rdata")
load("./tmp/news.sample.tdm.2g.Rdata")
load("./tmp/news.sample.tdm.3g.Rdata")

load("./tmp/news.sample.allTermsFrequency.1g.Rdata")
news.allTerms.1g <- corpora.allTermsFrequency

load("./tmp/news.sample.allTermsFrequency.2g.Rdata")
news.allTerms.2g <- corpora.allTermsFrequency

load("./tmp/news.sample.allTermsFrequency.3g.Rdata")
news.allTerms.3g <- corpora.allTermsFrequency
```

### 1-grams

```{r visualizeNewsData_1g, warning = F, message = F}
corpora.tdm <- news.corpora.tdm.1g

ft.lf.3500 <- findFreqTerms(corpora.tdm,lowfreq = 3500)

frequentTermsLimited.df <- getTermFrequencyInformationOrderedByTermFrequency(corpora.tdm, 3500)
visualizeBarPlot(ftm.df = frequentTermsLimited.df[-c(2,3),],titleBarPlot = "Frequent 1-grams")
visualizeWordcloud(ftm.df = frequentTermsLimited.df[-c(2,3),])
a <- getSomeInfoABoutCorpora(allFtm.df = news.allTerms.1g, filter = c(2,3))
```

__How many unique words do you need in a frequency sorted dictionary to cover 50% of all word instances in the language? 90%?__

| N = number of tokens | V = vocabulary size | 50% coverage | 90% coverage |
| ------------- |:-------------:| -----:|-----:|-----:|
| `r format(a$N, digits = 12, nsmall = 0)` | `r a$V` | `r a$C50` | `r a$C90` |

```{r}
visualizeCumuluativeCoverage(allFtm.df = news.allTerms.1g, title = "% Coverage By no of Unique Words (1-grams)", filter = c(2,3))
```

### 2-grams

```{r visualizeNewsData_2g, warning = F, message = F}
corpora.tdm <- news.corpora.tdm.2g

ft.lf.2000 <- findFreqTerms(corpora.tdm,lowfreq = 2000)

frequentTermsLimited.df <- getTermFrequencyInformationOrderedByTermFrequency(corpora.tdm, 2000)

visualizeBarPlot(ftm.df = frequentTermsLimited.df,titleBarPlot = "Frequent 2-grams")
visualizeWordcloud(ftm.df = frequentTermsLimited.df)
a <- getSomeInfoABoutCorpora(allFtm.df = news.allTerms.2g)
```

__How many unique words do you need in a frequency sorted dictionary to cover 50% of all word instances in the language? 90%?__

| N = number of tokens | V = vocabulary size | 50% coverage | 90% coverage |
| ------------- |:-------------:| -----:|-----:|-----:|
| `r format(a$N, digits = 12, nsmall = 0)` | `r a$V` | `r a$C50` | `r a$C90` |


```{r}
visualizeCumuluativeCoverage(allFtm.df = news.allTerms.2g, title = "% Coverage By no of Unique Words  (2-grams)")
```

### 3-grams

```{r visualizeNewsData_3g, warning = F, message = F}
corpora.tdm <- news.corpora.tdm.3g

ft.lf.300 <- findFreqTerms(corpora.tdm,lowfreq = 300)

frequentTermsLimited.df <- getTermFrequencyInformationOrderedByTermFrequency(corpora.tdm, 300)

visualizeBarPlot(ftm.df = frequentTermsLimited.df,titleBarPlot = "Frequent 3-grams")
visualizeWordcloud(ftm.df = frequentTermsLimited.df)
a <- getSomeInfoABoutCorpora(allFtm.df = news.allTerms.3g)
```

__How many unique words do you need in a frequency sorted dictionary to cover 50% of all word instances in the language? 90%?__

| N = number of tokens | V = vocabulary size | 50% coverage | 90% coverage |
| ------------- |:-------------:| -----:|-----:|-----:|
| `r format(a$N, digits = 12, nsmall = 0)` | `r a$V` | `r a$C50` | `r a$C90` |


```{r}
visualizeCumuluativeCoverage(allFtm.df = news.allTerms.3g, title = "% Coverage By no of Unique Words (3-grams)")
```