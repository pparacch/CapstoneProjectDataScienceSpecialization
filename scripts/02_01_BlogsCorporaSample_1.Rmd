---
title: "Blogs Corpora Creation"
author: "Pier Lorenzo Paracchini"
date: "4 mai 2016"
output: 
  html_document: 
    keep_md: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# WINDOWS LOCALE SETTING
Sys.setlocale(category = "LC_ALL",locale = "English_United States.1252")
Sys.getlocale("LC_CTYPE")
localeToCharset()
```

```{r loadTheData, eval = F}
rm(list = ls())
source("./corpora.R")

con <- file("./../data/original/final/en_US/en_US.blogs.txt", "r") 
data.blogs.all <- readLines(con, skipNul = T)
close(con)

data.blogs.all.nchar <- nchar(data.blogs.all)

data.blogs.all[200:203]

##Using iconv to replace non ASCII char with an empty char
data.blogs.all.ascii <-  iconv(data.blogs.all, from = localeToCharset(), to = "ASCII", "")


##Reset working data after removing Gremlings
data.blogs.all <- data.blogs.all.ascii
data.blogs.all.nchar <- nchar(data.blogs.all)

data.blogs.all[200:203]


#Let's focus on 
a <- data.blogs.all.nchar < 1300
par(mfrow=c(1,2))
hist(nchar(data.blogs.all[a]), main = "No Of Chars per Blogs (< 1300)", xlab = "no of characters")
hist(nchar(data.blogs.all[!a]), main = "No Of Chars per Blogs (>= 1300)", breaks = 10000, xlab = "no of characters")

shortBlogs.maxNoOfChar <- 20

a <- data.blogs.all.nchar <= shortBlogs.maxNoOfChar
head(data.blogs.all[a], 10)

#Removing such short blogss
data.blogs.all <- data.blogs.all[data.blogs.all.nchar > shortBlogs.maxNoOfChar]
data.blogs.all.nchar <- nchar(data.blogs.all)

head(data.blogs.all, 10)

## REmove Links
data.blogs.all <- remove_links(texts =  data.blogs.all)
data.blogs.all.nchar <- nchar(data.blogs.all)
tail(data.blogs.all)

## Remove Contractions
data.blogs.all <- remove_contractions(theTexts = data.blogs.all)
data.blogs.all.nchar <- nchar(data.blogs.all)
tail(data.blogs.all)

data.blogs.all.df <- data.frame(data.blogs.all)
save(data.blogs.all, data.blogs.all.df, file = "./tmp/blogs_step1_all.Rdata")
rm(list = ls())
```

```{r dataReduction, eval = F}
rm(list = ls())
source("corpora.R")
load(file = "./tmp/blogs_step1_all.Rdata")

set.seed(19711004)
coin.biased.outcome <- rbinom(length(data.blogs.all), 1, 0.05)
#table(coin.biased.outcome)
data.blogs.sample <- data.blogs.all[coin.biased.outcome == 1]

save(data.blogs.sample, file = "./tmp/blogs_step2_sample.Rdata")
rm(list = ls())
```

```{r generateTermDocumentMatrixAndSummaryHeavyProcessing1, cache = T, echo = F, eval = F}
rm(list = ls())
source("corpora.R")
load(file = "./tmp/blogs_step2_sample.Rdata")

#Unigrams
blogs.corpora.transformed <- corpora.transform(data.blogs.sample)
lapply(blogs.corpora.transformed[1:5], as.character)
lapply(blogs.corpora.transformed[100:110], as.character)
save(blogs.corpora.transformed, file = "./tmp/blogs.sample.corpora.transformed.Rdata")

blogs.corpora.tdm.1g <- tdm.generate.ng(blogs.corpora.transformed)
blogs.corpora.tdm.1g
lapply(blogs.corpora.transformed[1:5], as.character)
save(blogs.corpora.tdm.1g, file = "./tmp/blogs.sample.tdm.1g.Rdata")
rm(blogs.corpora.tdm.1g)


## Bi-grams
blogs.corpora.tdm.2g <- tdm.generate.ng(blogs.corpora.transformed,ng = 2)
blogs.corpora.tdm.2g
lapply(blogs.corpora.transformed[1:5], as.character)
save(blogs.corpora.tdm.2g, file = "./tmp/blogs.sample.tdm.2g.Rdata")
rm(blogs.corpora.tdm.2g)

## Tri-grams
blogs.corpora.tdm.3g <- tdm.generate.ng(blogs.corpora.transformed,ng = 3)
blogs.corpora.tdm.3g
lapply(blogs.corpora.transformed[1:5], as.character)
save(blogs.corpora.tdm.3g, file = "./tmp/blogs.sample.tdm.3g.Rdata")
rm(blogs.corpora.tdm.3g)

rm(list = ls())
```

```{r calculateTermFrequency, echo = F, eval = F}
rm(list = ls())
source("corpora.R")

##Blogs Corpora
load("./tmp/blogs.sample.tdm.1g.Rdata")
corpora.allTermsFrequency <- NULL
blogs.corpora.tdm.1g
corpora.tdm <- blogs.corpora.tdm.1g
corpora.allTermsFrequency <- getAllTermsFrequencyInCorpora.as.df(corpora.tdm = corpora.tdm, chunck = 2000)
save(corpora.allTermsFrequency, file = "./tmp/blogs.sample.allTermsFrequency.1g.Rdata")
rm(blogs.corpora.tdm.1g)

load("./tmp/blogs.sample.tdm.2g.Rdata")
corpora.allTermsFrequency <- NULL
blogs.corpora.tdm.2g
corpora.tdm <- blogs.corpora.tdm.2g
corpora.allTermsFrequency <- getAllTermsFrequencyInCorpora.as.df(corpora.tdm = corpora.tdm, chunck = 2000)
save(corpora.allTermsFrequency, file = "./tmp/blogs.sample.allTermsFrequency.2g.Rdata")
rm(blogs.corpora.tdm.2g)

load("./tmp/blogs.sample.tdm.3g.Rdata")
corpora.allTermsFrequency <- NULL
corpora.tdm <- blogs.corpora.tdm.3g
corpora.allTermsFrequency <- getAllTermsFrequencyInCorpora.as.df(corpora.tdm = corpora.tdm, chunck = 2000)
save(corpora.allTermsFrequency, file = "./tmp/blogs.sample.allTermsFrequency.3g.Rdata")
rm(blogs.corpora.tdm.3g)

corpora.allTermsFrequency <- NULL
corpora.tdm <- NULL

```

## Blogs Corpora

```{r loadBlogsData, warning = F, message = F}
rm(list = ls())
source("corpora.R")

load("./tmp/blogs.sample.tdm.1g.Rdata")
load("./tmp/blogs.sample.tdm.2g.Rdata")
load("./tmp/blogs.sample.tdm.3g.Rdata")

load("tmp/blogs.sample.allTermsFrequency.1g.Rdata")
blogs.allTerms.1g <- corpora.allTermsFrequency

load("tmp/blogs.sample.allTermsFrequency.2g.Rdata")
blogs.allTerms.2g <- corpora.allTermsFrequency

load("tmp/blogs.sample.allTermsFrequency.3g.Rdata")
blogs.allTerms.3g <- corpora.allTermsFrequency
```

### 1-grams

```{r visualizeBlogsData_1g, warning = F, message = F}
corpora.tdm <- blogs.corpora.tdm.1g

ft.lf.5000 <- findFreqTerms(corpora.tdm,lowfreq = 5000)

frequentTermsLimited.df <- getTermFrequencyInformationOrderedByTermFrequency(corpora.tdm, 5000)

visualizeBarPlot(ftm.df = frequentTermsLimited.df[-c(3,4),],titleBarPlot = "Frequent 1-grams")
visualizeWordcloud(ftm.df = frequentTermsLimited.df[-c(3,4),])
a <- getSomeInfoABoutCorpora(allFtm.df = blogs.allTerms.1g, filter = c(3,4))
```

__How many unique words do you need in a frequency sorted dictionary to cover 50% of all word instances in the language? 90%?__

| N = number of tokens | V = vocabulary size | 50% coverage | 90% coverage |
| ------------- |:-------------:| -----:|-----:|-----:|
| `r format(a$N, digits = 12, nsmall = 0)` | `r a$V` | `r a$C50` | `r a$C90` |


```{r}
visualizeCumuluativeCoverage(allFtm.df = blogs.allTerms.1g, title = "% Coverage By no of Unique Words (1-grams)", filter = c(3,4))
```

### 2-grams

```{r visualizeBlogsData_2g, warning = F, message = F}
corpora.tdm <- blogs.corpora.tdm.2g

ft.lf.2500 <- findFreqTerms(corpora.tdm,lowfreq = 2500)

frequentTermsLimited.df <- getTermFrequencyInformationOrderedByTermFrequency(corpora.tdm, 2500)

visualizeBarPlot(ftm.df = frequentTermsLimited.df,titleBarPlot = "Frequent 2-grams")
visualizeWordcloud(ftm.df = frequentTermsLimited.df)
a <- getSomeInfoABoutCorpora(allFtm.df = blogs.allTerms.2g)
```

__How many unique words do you need in a frequency sorted dictionary to cover 50% of all word instances in the language? 90%?__

| N = number of tokens | V = vocabulary size | 50% coverage | 90% coverage |
| ------------- |:-------------:| -----:|-----:|-----:|
| `r format(a$N, digits = 12, nsmall = 0)` | `r a$V` | `r a$C50` | `r a$C90` |


```{r}
visualizeCumuluativeCoverage(allFtm.df = blogs.allTerms.2g, title = "% Coverage By no of Unique Words (2-grams)")
```

### 3-grams

```{r visualizeBlogsData_3g, warning = F, message = F}
corpora.tdm <- blogs.corpora.tdm.3g

ft.lf.300 <- findFreqTerms(corpora.tdm,lowfreq = 300)

frequentTermsLimited.df <- getTermFrequencyInformationOrderedByTermFrequency(corpora.tdm, 300)

visualizeBarPlot(ftm.df = frequentTermsLimited.df,titleBarPlot = "Frequent 3-grams")
visualizeWordcloud(ftm.df = frequentTermsLimited.df)
a <- getSomeInfoABoutCorpora(allFtm.df = blogs.allTerms.3g)
```

__How many unique words do you need in a frequency sorted dictionary to cover 50% of all word instances in the language? 90%?__

| N = number of tokens | V = vocabulary size | 50% coverage | 90% coverage |
| ------------- |:-------------:| -----:|-----:|-----:|
| `r format(a$N, digits = 12, nsmall = 0)` | `r a$V` | `r a$C50` | `r a$C90` |


```{r}
visualizeCumuluativeCoverage(allFtm.df = blogs.allTerms.3g, title = "% Coverage By no of Unique Words (3-grams)")
```